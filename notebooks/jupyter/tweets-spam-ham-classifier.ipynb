{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Spam-Ham Classifier\n",
    "## Author: Luis Eduardo Ferro Diez <a href=\"luisedof10@gmail.com\">luisedof10@gmail.com</a>, <a href='mailto:luis.ferro1@correo.icesi.edu.co'>luis.ferro1@correo.icesi.edu.co</a>\n",
    "\n",
    "This notebook contains the steps taken to train a model to classify spam-ham tweets in Scala-Spark for later usage in the Where to Sell Products project.\n",
    "\n",
    "## Dataset\n",
    "* https://www.kaggle.com/c/twitter-spam/data\n",
    "\n",
    "## Resources\n",
    "* https://www.kaggle.com/c/twitter-spam/overview\n",
    "* https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = 2.4.4\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basePath: String = /media/ohtar10/Adder-Storage/datasets/twitter/spam-ham/twitter-spam\n",
      "training: String = /media/ohtar10/Adder-Storage/datasets/twitter/spam-ham/twitter-spam/train.csv\n",
      "testing: String = /media/ohtar10/Adder-Storage/datasets/twitter/spam-ham/twitter-spam/test.csv\n"
     ]
    }
   ],
   "source": [
    "val basePath = \"/media/ohtar10/Adder-Storage/datasets/twitter/spam-ham/twitter-spam\"\n",
    "val training = basePath + \"/train.csv\"\n",
    "val testing = basePath + \"/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.functions._\n",
      "import spark.implicits._\n",
      "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,LongType,false), StructField(Tweet,StringType,false), StructField(following,DoubleType,true), StructField(followers,DoubleType,true), StructField(actions,DoubleType,true), StructField(is_retweet,DoubleType,true), StructField(location,StringType,true), StructField(Type,StringType,false))\n",
      "trainingDF: org.apache.spark.sql.DataFrame = [id: bigint, tweet: string ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = new StructType().\n",
    "    add(\"Id\", LongType, false).\n",
    "    add(\"Tweet\",StringType, false).\n",
    "    add(\"following\",DoubleType, true).\n",
    "    add(\"followers\",DoubleType, true).\n",
    "    add(\"actions\", DoubleType, true).\n",
    "    add(\"is_retweet\", DoubleType, true).\n",
    "    add(\"location\", StringType, true).\n",
    "    add(\"Type\", StringType, false)\n",
    "  \n",
    "val trainingDF = spark.read.\n",
    "    option(\"header\", true).\n",
    "    schema(schema).\n",
    "    csv(training).\n",
    "    select($\"Id\".as(\"id\"), \n",
    "            $\"Tweet\".as(\"tweet\"), \n",
    "            $\"following\", \n",
    "            $\"followers\", \n",
    "            $\"actions\", \n",
    "            $\"is_retweet\", \n",
    "            $\"location\",\n",
    "            when($\"location\".isNull, 0.0).otherwise(1.0).alias(\"has_location\"),\n",
    "            $\"Type\".as(\"type\")).where(\"Type is not null\").\n",
    "    na.fill(0.0, Seq(\"following\", \"followers\", \"actions\", \"is_retweet\"))\n",
    "\n",
    "trainingDF.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3092f06c555485aa6d82b275685fdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a04b80d393400391303eb963db3472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from tweets limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- following: double (nullable = false)\n",
      " |-- followers: double (nullable = false)\n",
      " |-- actions: double (nullable = false)\n",
      " |-- is_retweet: double (nullable = false)\n",
      " |-- location: string (nullable = true)\n",
      " |-- has_location: double (nullable = false)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the 'Type' field corresponds to a binary class: \"Spam\" for spam tweets and \"Quality\" for genuine tweets.\n",
    "\n",
    "We also observe that as features we can use:\n",
    "\n",
    "* Tweet: The tweet text\n",
    "* following: The amount of accounts the author of this particular tweet is following\n",
    "* followers: The amount of followers the author of this particalr tweet has\n",
    "* actions: The total amount of favourites, retweets and replies this particular tweet has\n",
    "* is_reweet: Binary where 0 means the tweet is not a retweet and 1 that it is a retweet.\n",
    "* location: The user provided location for the account.\n",
    "\n",
    "For the non-text attributes we could train a simple classification model. However, since we are dealing with text, we need to incorporate some NLP techniques to create a feature vector out of the tweet text.\n",
    "\n",
    "We have two options: we can either implement a stemming + wordcount algorithm, probably removing the stop words to build the feature vector. Or, we can train a LDA to discover topics among the text corpora and associate them with the target class.\n",
    "\n",
    "Let's first create a feature vector with the non-text attributes. We will create a spark pipeline for this\n",
    "\n",
    "### First let's train a TF based model\n",
    "This is, the features from text will be taken according to the term frequency of the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.feature.{StringIndexer, Tokenizer, StopWordsRemover, HashingTF, VectorAssembler}\n",
      "import org.apache.spark.ml.classification.LogisticRegression\n",
      "classIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_1706446d5e54\n",
      "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_7f5be71bdee6\n",
      "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d625072334cb\n",
      "hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_701ec6c467d9\n",
      "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e92231775552\n",
      "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_5f3632ea871a\n",
      "hashingTFPipeline: org.apache.spark.ml.Pipeline = pipeline_1b96de29838f\n",
      "predictions: org.apache.spark.sql.DataFrame = [id: bigint, tweet: string ... 15 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{StringIndexer, Tokenizer, StopWordsRemover, HashingTF, VectorAssembler}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "//To transform the string classes to a binary-numerical representation\n",
    "val classIndexer = new StringIndexer().\n",
    "    setInputCol(\"type\").\n",
    "    setOutputCol(\"label\")\n",
    "    //.setStringOrderType(\"alphabetDesc\") //Asign 1 to Quality and 0 to Spam (Alphabetical order) NOTE: This is not supported in spark 2.2.1\n",
    "\n",
    "//To transform the text of the tweets into tokens (array of words)    \n",
    "val tokenizer = new Tokenizer().\n",
    "    setInputCol(\"tweet\").\n",
    "    setOutputCol(\"text_tokens\")\n",
    "\n",
    "//To remove stop words like pronouns and topic marking particles from the text    \n",
    "val stopWordsRemover = new StopWordsRemover().\n",
    "    setInputCol(\"text_tokens\").\n",
    "    setOutputCol(\"text_filtered\").\n",
    "    setLocale(\"en_US\") // Because most of the english tweets comes from US\n",
    "\n",
    "//To calculate the term frequencies    \n",
    "val hashingTF = new HashingTF().\n",
    "    setNumFeatures(1000).\n",
    "    setInputCol(\"text_filtered\").\n",
    "    setOutputCol(\"text_features\")\n",
    "    \n",
    "//To assemble all the features into a single feature vector\n",
    "val vectorAssembler = new VectorAssembler().\n",
    "    setInputCols(Array(\"following\", \"followers\", \"actions\", \"is_retweet\", \"has_location\", \"text_features\")).\n",
    "    setOutputCol(\"features\")\n",
    "\n",
    "//To train the binary classifier for spam-vs-ham tweets    \n",
    "val lr = new LogisticRegression().\n",
    "    setMaxIter(100).\n",
    "    setRegParam(0.001)\n",
    "    \n",
    "val hashingTFPipeline = new Pipeline().\n",
    "    setStages(\n",
    "        Array(\n",
    "            classIndexer, \n",
    "            tokenizer, \n",
    "            stopWordsRemover, \n",
    "            hashingTF,\n",
    "            vectorAssembler,\n",
    "            lr\n",
    "            ))\n",
    "    \n",
    "val predictions = hashingTFPipeline.fit(trainingDF).transform(trainingDF)\n",
    "predictions.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d8b38c897b49969ee4e3bf3bee5891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a23efd1adb411984a00a60f9766b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select id, tweet, following, followers, actions, is_retweet, has_location, label, prediction, probability from tweets limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_2463086e69a8\n",
      "roc: Double = 0.9902136603802713\n",
      "Area Under ROC 0.9902136603802713\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().\n",
    "    setMetricName(\"areaUnderROC\")\n",
    "    \n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"Area Under ROC ${roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a model capable of predicting 96% of the classes in the training set. There is a chance of overfitting and a CrossValidation technique will be good to implement to ensure we are training a good model.\n",
    "\n",
    "### Now let's train a Word2Vec based model\n",
    "This is, instead of accounting for the term frequencies of the tweet's text, we will transform each tweet into a vector which has the capabilities of preserving the semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.Word2Vec\n",
      "word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_0e4216602462\n",
      "word2VecPipeline: org.apache.spark.ml.Pipeline = pipeline_466822aa3015\n",
      "predictions: org.apache.spark.sql.DataFrame = [id: bigint, tweet: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "\n",
    "val word2Vec = new Word2Vec().\n",
    "    setVectorSize(20).\n",
    "    setMinCount(0).\n",
    "    setInputCol(\"text_tokens\").\n",
    "    setOutputCol(\"text_features\")\n",
    "    \n",
    "val word2VecPipeline = new Pipeline().\n",
    "    setStages(\n",
    "        Array(\n",
    "            classIndexer, \n",
    "            tokenizer, \n",
    "            //stopWordsRemover, \n",
    "            word2Vec,\n",
    "            vectorAssembler,\n",
    "            lr\n",
    "            ))\n",
    "    \n",
    "val predictions = word2VecPipeline.fit(trainingDF).transform(trainingDF)\n",
    "predictions.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7151ecb992ae480099c75ef8259dbd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8a3dbbc90044a69dbb9ceb46871a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select id, tweet, following, followers, actions, is_retweet, has_location, label, prediction, probability from tweets limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_45d5de131c5f\n",
      "roc: Double = 0.959969816693628\n",
      "Area Under ROC 0.959969816693628\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().\n",
    "    setMetricName(\"areaUnderROC\")\n",
    "    \n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"Area Under ROC ${roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 95% in the ROC curve which is slightly worse than the TF appdoach. However, we might be facing overfitting, hence probably the Word2Vec model generalizes better.\n",
    "\n",
    "In both cases however, there is risk of overfitting, hence let’s perform a Cross Validation with the training set to ensure we are constructing good models.\n",
    "Note:\n",
    "\n",
    "These param grids are intended to find the best parameters for the model training. The caveat with the param grid is that the amount of model to train grows significantly and train this in a commodity machine can be troublesome. We executed this on EMR on a 3 node cluster, 1 master node (m4.xlarge: 4 vCPU, 16GB Ram) and 2 workers (m4.2xlarge: 8 vCPU, 32GB Ram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
      "hashingTFParamGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter:...word2VecParamGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1,\n",
      "\tw2v_0e4216602462-vectorSize: 25\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01,\n",
      "\tw2v_0e4216602462-vectorSize: 25\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001,\n",
      "\tw2v_0e4216602462-vectorSize: 25\n",
      "}, {\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1,\n",
      "\tw..."
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val hashingTFParamGrid = new ParamGridBuilder().\n",
    "    addGrid(hashingTF.numFeatures, Array(10, 25, 50)).\n",
    "    addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).\n",
    "    addGrid(lr.maxIter, Array(10, 50, 100)).\n",
    "    build()\n",
    "\n",
    "val word2VecParamGrid = new ParamGridBuilder().\n",
    "    addGrid(word2Vec.vectorSize, Array(10, 25, 50)).\n",
    "    addGrid(lr.regParam, Array(0.1, 0.01, 0.001)).\n",
    "    addGrid(lr.maxIter, Array(10, 50, 100)).\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.tuning.CrossValidator\n",
      "hashingTFCV: org.apache.spark.ml.tuning.CrossValidator = cv_682b377784ad\n",
      "hashingTFCModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_682b377784ad\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "//First the hashingTFPipeline\n",
    "val hashingTFCV = new CrossValidator().\n",
    "    setEstimator(hashingTFPipeline).\n",
    "    setEvaluator(evaluator).\n",
    "    setEstimatorParamMaps(hashingTFParamGrid).\n",
    "    setNumFolds(5).\n",
    "    setParallelism(2)\n",
    "\n",
    "val hashingTFCModel = hashingTFCV.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [id: bigint, tweet: string ... 15 more fields]\n",
      "roc: Double = 0.9664722517796072\n",
      "Area under ROC 0.9664722517796072\n"
     ]
    }
   ],
   "source": [
    "val predictions = hashingTFCModel.transform(trainingDF)\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC ${roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cross validation we got 97% with the TF based model.\n",
    "\n",
    "Now let’s do the same for the word2vec based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vecCV: org.apache.spark.ml.tuning.CrossValidator = cv_ccb786b81da8\n",
      "word2VecCVModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_ccb786b81da8\n"
     ]
    }
   ],
   "source": [
    "//Then the word 2 vec pipeline\n",
    "val word2vecCV = new CrossValidator().\n",
    "    setEstimator(word2VecPipeline).\n",
    "    setEvaluator(evaluator).\n",
    "    setEstimatorParamMaps(word2VecParamGrid).\n",
    "    setNumFolds(5).\n",
    "    setParallelism(2)\n",
    "  \n",
    "val word2VecCVModel = word2vecCV.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [id: bigint, tweet: string ... 14 more fields]\n",
      "roc: Double = 0.8088541597515011\n",
      "Area under ROC 0.8088541597515011\n"
     ]
    }
   ],
   "source": [
    "val predictions = word2VecCVModel.transform(trainingDF)\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC ${roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 80% in the evaluation, this suggests that the word2Vec model was overfitting the data, however, we might still want to use it as this is a good score.\n",
    "\n",
    "Let’s explore the model best parameters\n",
    "\n",
    "#### For the HashingTF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.PipelineModel\n",
      "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
      "avgMetricsParamGrid: Array[Double] = Array(0.9145250179920914, 0.9448457169665666, 0.968437156273551, 0.9144340201894247, 0.9431491259476411, 0.969148484115817, 0.9144340201894247, 0.9431491259476411, 0.969148484115817, 0.9155308768247561, 0.9461308609499272, 0.9678906547940839, 0.9154269734203883, 0.9432459497743395, 0.9689206232983162, 0.9154269734203883, 0.9432459497743395, 0.9689206232983162, 0.9166665035906965, 0.9477557882782236, 0.967594423181924, 0.9165523694940602, 0.943738096843964, 0.9688179240571554, 0.9165523694940602, 0.943738096843964, 0.9688180852104976)\n",
      "combined: Array[(org.apache.spark.ml.param.ParamMap, Double)] =\n",
      "Array(({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1\n",
      "},0.9145250179920914), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01\n",
      "},0.9448457169665666), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001\n",
      "},0.968437156273551), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1\n",
      "},0.9144340201894247), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashingTF_701ec6c467d9-numFeatures: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01\n",
      "},0.9431491259476411), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 50,\n",
      "\thashin...bestModel: org.apache.spark.ml.PipelineModel = pipeline_1b96de29838f\n",
      "bestHashingTFNumFeatures: String =\n",
      "binary: If true, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts (default: false)\n",
      "inputCol: input column name (current: text_filtered)\n",
      "numFeatures: number of features (> 0) (default: 262144, current: 10)\n",
      "outputCol: output column name (default: hashingTF_701ec6c467d9__output, current: text_features)\n",
      "bestLRParams: String =\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained o...##########\n",
      "The best HashingTF number of features are: \n",
      "binary: If true, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts (default: false)\n",
      "inputCol: input column name (current: text_filtered)\n",
      "numFeatures: number of features (> 0) (default: 262144, current: 10)\n",
      "outputCol: output column name (default: hashingTF_701ec6c467d9__output, current: text_features)\n",
      "##########\n",
      "The best logistic regression parameters are:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 50)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 0.001)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.PipelineModel\n",
    "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
    "\n",
    "//Average metrics per param grid\n",
    "val avgMetricsParamGrid = hashingTFCModel.avgMetrics\n",
    "\n",
    "//Combibe with param grid to see how they affect the overall metrics\n",
    "val combined = hashingTFParamGrid.zip(avgMetricsParamGrid)\n",
    "\n",
    "val bestModel = hashingTFCModel.bestModel.asInstanceOf[PipelineModel]\n",
    "\n",
    "//Explain params for each stage\n",
    "val bestHashingTFNumFeatures = bestModel.stages(3).asInstanceOf[HashingTF].explainParams\n",
    "val bestLRParams = bestModel.stages(5).asInstanceOf[LogisticRegressionModel].explainParams\n",
    "\n",
    "println(\"#\" * 10)\n",
    "println(s\"The best HashingTF number of features are: \")\n",
    "println(bestHashingTFNumFeatures)\n",
    "println(\"#\" * 10)\n",
    "println(s\"The best logistic regression parameters are:\")\n",
    "println(bestLRParams)\n",
    "println(\"#\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for the HashingTF suggest that the CV can be pushed further.\n",
    "\n",
    "Now let's check the word2Vec ones.\n",
    "\n",
    "#### For the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.Word2VecModel\n",
      "avgMetricsParamGrid: Array[Double] = Array(0.9267040769817643, 0.9499469659313368, 0.9637291691154359, 0.9233550984678844, 0.9376269136248313, 0.9430189010981028, 0.9125169177937366, 0.928507508086202, 0.9314878700689965, 0.9267478314072566, 0.9493011969665435, 0.9718207935668939, 0.9248641405703333, 0.9499991035500553, 0.9725179610133754, 0.9229259391870261, 0.949949442027334, 0.9737005706383913, 0.9267478314072566, 0.9493011969665435, 0.9718208407830964, 0.9248641405703333, 0.9499973538317772, 0.9724977999705011, 0.9229259391870261, 0.9499672539914416, 0.9735091760481087)\n",
      "combined: Array[(org.apache.spark.ml.param.ParamMap, Double)] =\n",
      "Array(({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "},0.9267040769817643), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "},0.9499469659313368), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001,\n",
      "\tw2v_0e4216602462-vectorSize: 10\n",
      "},0.9637291691154359), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.1,\n",
      "\tw2v_0e4216602462-vectorSize: 25\n",
      "},0.9233550984678844), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.01,\n",
      "\tw2v_0e4216602462-vectorSize: 25\n",
      "},0.9376269136248313), ({\n",
      "\tlogreg_5f3632ea871a-maxIter: 10,\n",
      "\tlogreg_5f3632ea871a-regParam: 0.001,\n",
      "\tw2...bestModel: org.apache.spark.ml.PipelineModel = pipeline_466822aa3015\n",
      "bestWord2VecFeatures: String =\n",
      "inputCol: input column name (current: text_tokens)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 1)\n",
      "maxSentenceLength: Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks up to the size (> 0) (default: 1000)\n",
      "minCount: the minimum number of times a token must appear to be included in the word2vec model's vocabulary (>= 0) (default: 5, current: 0)\n",
      "numPartitions: number of partitions for sentences of words (> 0) (default: 1)\n",
      "outputCol: output column name (default: w2v_0e4216602462__output, current: text_features)\n",
      "seed: random seed (default: -1961189076)\n",
      "stepSize: Step size to be used for each iteration of optimization (> 0) (default: 0.025)\n",
      "vectorSize: the dimension of code...bestLRParams: String =\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained o...##########\n",
      "The best Word2Vec features are: \n",
      "inputCol: input column name (current: text_tokens)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 1)\n",
      "maxSentenceLength: Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks up to the size (> 0) (default: 1000)\n",
      "minCount: the minimum number of times a token must appear to be included in the word2vec model's vocabulary (>= 0) (default: 5, current: 0)\n",
      "numPartitions: number of partitions for sentences of words (> 0) (default: 1)\n",
      "outputCol: output column name (default: w2v_0e4216602462__output, current: text_features)\n",
      "seed: random seed (default: -1961189076)\n",
      "stepSize: Step size to be used for each iteration of optimization (> 0) (default: 0.025)\n",
      "vectorSize: the dimension of codes after transforming from words (> 0) (default: 100, current: 50)\n",
      "windowSize: the window size (context words from [-window, window]) (> 0) (default: 5)\n",
      "##########\n",
      "The best logistic regression parameters are:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 50)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 0.001)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2VecModel\n",
    "\n",
    "//Average metrics per param grid\n",
    "val avgMetricsParamGrid = word2VecCVModel.avgMetrics\n",
    "\n",
    "//Combibe with param grid to see how they affect the overall metrics\n",
    "val combined = word2VecParamGrid.zip(avgMetricsParamGrid)\n",
    "\n",
    "val bestModel = word2VecCVModel.bestModel.asInstanceOf[PipelineModel]\n",
    "\n",
    "//Explain params for each stage\n",
    "val bestWord2VecFeatures = bestModel.stages(2).asInstanceOf[Word2VecModel].explainParams\n",
    "val bestLRParams = bestModel.stages(4).asInstanceOf[LogisticRegressionModel].explainParams\n",
    "\n",
    "println(\"#\" * 10)\n",
    "println(s\"The best Word2Vec features are: \")\n",
    "println(bestWord2VecFeatures)\n",
    "println(\"#\" * 10)\n",
    "println(s\"The best logistic regression parameters are:\")\n",
    "println(bestLRParams)\n",
    "println(\"#\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the word2vec model, we are also at the edge of the number of dimensions, which suggests that we can indeed push further to validate a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
