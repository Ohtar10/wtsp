{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Language Classification\n",
    "\n",
    "## Author: Luis Eduardo Ferro Diez <a href=\"mailto:luis.ferro1@correo.icesi.edu.co\">luis.ferro1@correo.icesi.edu.co</a>\n",
    "\n",
    "This notebook contains the model training preparation for classifying the tweets language.\n",
    "\n",
    "## Dataset\n",
    "* Tweets dataset in parquet format (After executing the first transformation spark pipeline)\n",
    "\n",
    "## Resources\n",
    "* https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = \"../../datasets/tweets_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_parquet(tweets_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_following_count</th>\n",
       "      <th>...</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>country</th>\n",
       "      <th>country_code</th>\n",
       "      <th>place_type</th>\n",
       "      <th>place_url</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>374048987034419200</td>\n",
       "      <td>@adambeyer234: I already miss Jace like hell</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>363516745</td>\n",
       "      <td>rilez_sharp</td>\n",
       "      <td>582.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>...</td>\n",
       "      <td>New York, US</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>admin</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/94965b2c453...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>374048991224160256</td>\n",
       "      <td>I really really hate texting unless we're talk...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>542867684</td>\n",
       "      <td>ivonne_xoxo</td>\n",
       "      <td>367.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>city</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/1d9a5370a35...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374048995414659072</td>\n",
       "      <td>Wind 4.0 mph SSE. Barometer 1040.0 mb, Falling...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1035302827</td>\n",
       "      <td>MossleyWX</td>\n",
       "      <td>38.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Craven, North Yorkshire</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>GB</td>\n",
       "      <td>city</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/4e008be7a8d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374048999642120192</td>\n",
       "      <td>\"@3gerardpique: Congratulations to Bayern Münc...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1716178933</td>\n",
       "      <td>456ronnys</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Cakung, Jakarta Timur</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>city</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/ac9f3b0d4a9...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>374048999625723904</td>\n",
       "      <td>@612wildabeast ima do that brah lol</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373470202</td>\n",
       "      <td>BrandonWarren40</td>\n",
       "      <td>75.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>city</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/dc62519fda1...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet lang  \\\n",
       "0  374048987034419200       @adambeyer234: I already miss Jace like hell   en   \n",
       "1  374048991224160256  I really really hate texting unless we're talk...   en   \n",
       "2  374048995414659072  Wind 4.0 mph SSE. Barometer 1040.0 mb, Falling...   en   \n",
       "3  374048999642120192  \"@3gerardpique: Congratulations to Bayern Münc...   en   \n",
       "4  374048999625723904                @612wildabeast ima do that brah lol   en   \n",
       "\n",
       "   favorite_count  retweet_count  is_retweet     user_id        user_name  \\\n",
       "0             0.0            0.0         0.0   363516745      rilez_sharp   \n",
       "1             0.0            0.0         0.0   542867684      ivonne_xoxo   \n",
       "2             0.0            0.0         0.0  1035302827        MossleyWX   \n",
       "3             0.0            0.0         0.0  1716178933        456ronnys   \n",
       "4             0.0            0.0         0.0   373470202  BrandonWarren40   \n",
       "\n",
       "   user_followers_count  user_following_count  ...          place_full_name  \\\n",
       "0                 582.0                 666.0  ...             New York, US   \n",
       "1                 367.0                 310.0  ...              Chicago, IL   \n",
       "2                  38.0                  60.0  ...  Craven, North Yorkshire   \n",
       "3                   6.0                  65.0  ...    Cakung, Jakarta Timur   \n",
       "4                  75.0                  36.0  ...                Tampa, FL   \n",
       "\n",
       "          country country_code place_type  \\\n",
       "0   United States           US      admin   \n",
       "1   United States           US       city   \n",
       "2  United Kingdom           GB       city   \n",
       "3       Indonesia           ID       city   \n",
       "4   United States           US       city   \n",
       "\n",
       "                                           place_url is_spam  year month day  \\\n",
       "0  https://api.twitter.com/1.1/geo/id/94965b2c453...     0.0  2013     9   1   \n",
       "1  https://api.twitter.com/1.1/geo/id/1d9a5370a35...     0.0  2013     9   1   \n",
       "2  https://api.twitter.com/1.1/geo/id/4e008be7a8d...     0.0  2013     9   1   \n",
       "3  https://api.twitter.com/1.1/geo/id/ac9f3b0d4a9...     0.0  2013     9   1   \n",
       "4  https://api.twitter.com/1.1/geo/id/dc62519fda1...     0.0  2013     9   1   \n",
       "\n",
       "  hour  \n",
       "0    1  \n",
       "1    1  \n",
       "2    1  \n",
       "3    1  \n",
       "4    1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the most relevant parts to predict the tweet language to be:\n",
    "* The tweet text\n",
    "* The country code\n",
    "\n",
    "The tweet text might contain user mentions, for simplicity, we will first transform the text, replacing the user mention with the text \"@usermention\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@usermention: I already miss Jace like hell</td>\n",
       "      <td>US</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really really hate texting unless we're talk...</td>\n",
       "      <td>US</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wind 4.0 mph SSE. Barometer 1040.0 mb, Falling...</td>\n",
       "      <td>GB</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@usermention: Congratulations to Bayern Münch...</td>\n",
       "      <td>ID</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@usermention ima do that brah lol</td>\n",
       "      <td>US</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet country_code lang\n",
       "0        @usermention: I already miss Jace like hell           US   en\n",
       "1  I really really hate texting unless we're talk...           US   en\n",
       "2  Wind 4.0 mph SSE. Barometer 1040.0 mb, Falling...           GB   en\n",
       "3  \"@usermention: Congratulations to Bayern Münch...           ID   en\n",
       "4                  @usermention ima do that brah lol           US   en"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tweets.tweet = tweets.tweet.apply(lambda x: re.sub(r\"@[\\w\\d]+\", \"@usermention\", x))\n",
    "tweets = tweets[[\"tweet\", \"country_code\", \"lang\"]]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.lang.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information we'll create embeddings to train a CNN + Fully Connected ANN to predict the language.\n",
    "\n",
    "Also for testing, we are going to use a LTSM + Fully Connected ANN and compare the results.\n",
    "\n",
    "First, let's split the dataset into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(tweets, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "words = 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(tweets.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'usermention': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'the': 4,\n",
       " 't': 5,\n",
       " 'co': 6,\n",
       " 'me': 7,\n",
       " 'you': 8,\n",
       " 'and': 9,\n",
       " 'a': 10,\n",
       " 'of': 11,\n",
       " 'my': 12,\n",
       " 'for': 13,\n",
       " 'in': 14,\n",
       " 'with': 15,\n",
       " 'http': 16,\n",
       " 'be': 17,\n",
       " 'it': 18,\n",
       " '0': 19,\n",
       " '—': 20,\n",
       " 'https': 21,\n",
       " 'that': 22,\n",
       " 'is': 23,\n",
       " 'but': 24,\n",
       " 'at': 25,\n",
       " 'lol': 26,\n",
       " 'by': 27,\n",
       " 'so': 28,\n",
       " 'she': 29,\n",
       " 'all': 30,\n",
       " 'im': 31,\n",
       " 'up': 32,\n",
       " 'go': 33,\n",
       " 'should': 34,\n",
       " 'when': 35,\n",
       " 'this': 36,\n",
       " 'on': 37,\n",
       " \"don't\": 38,\n",
       " 'today': 39,\n",
       " 'love': 40,\n",
       " 'u': 41,\n",
       " 'do': 42,\n",
       " 'now': 43,\n",
       " 'text': 44,\n",
       " 'your': 45,\n",
       " 'come': 46,\n",
       " 'off': 47,\n",
       " 'work': 48,\n",
       " 'hard': 49,\n",
       " 'home': 50,\n",
       " 'out': 51,\n",
       " 'been': 52,\n",
       " 'hoes': 53,\n",
       " 'have': 54,\n",
       " 'was': 55,\n",
       " 'pic': 56,\n",
       " 'like': 57,\n",
       " 'really': 58,\n",
       " 'hate': 59,\n",
       " 'wind': 60,\n",
       " 'rain': 61,\n",
       " 'just': 62,\n",
       " 'give': 63,\n",
       " 'good': 64,\n",
       " 'music': 65,\n",
       " 'famous': 66,\n",
       " 'dessert': 67,\n",
       " 'high': 68,\n",
       " 'day': 69,\n",
       " 'lmao': 70,\n",
       " 'birthday': 71,\n",
       " 'would': 72,\n",
       " 'her': 73,\n",
       " 'plz': 74,\n",
       " '😂': 75,\n",
       " 'worth': 76,\n",
       " 'k': 77,\n",
       " 'only': 78,\n",
       " 'take': 79,\n",
       " 'lazy': 80,\n",
       " 'world': 81,\n",
       " '3': 82,\n",
       " 'never': 83,\n",
       " 'ask': 84,\n",
       " 'fall': 85,\n",
       " 'they': 86,\n",
       " 'get': 87,\n",
       " 'back': 88,\n",
       " 'doing': 89,\n",
       " 'stuff': 90,\n",
       " 'want': 91,\n",
       " \"i'm\": 92,\n",
       " 'aint': 93,\n",
       " 'sooo': 94,\n",
       " 'tired': 95,\n",
       " 'mine': 96,\n",
       " 'shit': 97,\n",
       " 'an': 98,\n",
       " 'make': 99,\n",
       " 'last': 100,\n",
       " 'blast': 101,\n",
       " 'house': 102,\n",
       " 'made': 103,\n",
       " \"it's\": 104,\n",
       " 'night': 105,\n",
       " 'already': 106,\n",
       " 'miss': 107,\n",
       " 'jace': 108,\n",
       " 'hell': 109,\n",
       " 'texting': 110,\n",
       " 'unless': 111,\n",
       " \"we're\": 112,\n",
       " 'talking': 113,\n",
       " 'about': 114,\n",
       " 'something': 115,\n",
       " 'important': 116,\n",
       " '4': 117,\n",
       " 'mph': 118,\n",
       " 'sse': 119,\n",
       " 'barometer': 120,\n",
       " '1040': 121,\n",
       " 'mb': 122,\n",
       " 'falling': 123,\n",
       " 'slowly': 124,\n",
       " 'temperature': 125,\n",
       " '11': 126,\n",
       " '7': 127,\n",
       " '°c': 128,\n",
       " 'mm': 129,\n",
       " 'humidity': 130,\n",
       " '72': 131,\n",
       " 'congratulations': 132,\n",
       " 'bayern': 133,\n",
       " 'münchen': 134,\n",
       " 'pep': 135,\n",
       " 'guardiola': 136,\n",
       " 'winning': 137,\n",
       " 'european': 138,\n",
       " 'super': 139,\n",
       " 'cup': 140,\n",
       " 'football': 141,\n",
       " 'ima': 142,\n",
       " 'brah': 143,\n",
       " 'reason': 144,\n",
       " 'feat': 145,\n",
       " 'nate': 146,\n",
       " 'ruess': 147,\n",
       " 'p': 148,\n",
       " 'nk': 149,\n",
       " 'nqjo5zyxxe': 150,\n",
       " 'cleeve': 151,\n",
       " 'cheltenham': 152,\n",
       " 'ukweather': 153,\n",
       " '0km': 154,\n",
       " 'h': 155,\n",
       " 'pres': 156,\n",
       " '1032': 157,\n",
       " '0mb': 158,\n",
       " 'steady': 159,\n",
       " 'temp': 160,\n",
       " '9': 161,\n",
       " '3c': 162,\n",
       " '0mm': 163,\n",
       " 'humid': 164,\n",
       " '84': 165,\n",
       " 'sun': 166,\n",
       " '07': 167,\n",
       " '00': 168,\n",
       " 'enjoyment': 169,\n",
       " 'amp': 170,\n",
       " 'afternoon': 171,\n",
       " '😊': 172,\n",
       " 'weekend': 173,\n",
       " 'hightea': 174,\n",
       " 'society': 175,\n",
       " 'café': 176,\n",
       " 'pjuejg7flc': 177,\n",
       " 'united': 178,\n",
       " 'chan': 179,\n",
       " '20': 180,\n",
       " 'cougar': 181,\n",
       " 'could': 182,\n",
       " 'wish': 183,\n",
       " 'happy': 184,\n",
       " \"that's\": 185,\n",
       " 'plz😊': 186,\n",
       " 'cliff': 187,\n",
       " 'man': 188,\n",
       " 'early': 189,\n",
       " 'start': 190,\n",
       " 'going': 191,\n",
       " 'buzzing': 192,\n",
       " 'races': 193,\n",
       " 'silverstone': 194,\n",
       " 'nw': 195,\n",
       " 'pair': 196,\n",
       " 'king': 197,\n",
       " 'iloveyou': 198,\n",
       " 'he': 199,\n",
       " 'hates': 200,\n",
       " 'expect': 201,\n",
       " 'couple': 202,\n",
       " \"dm's\": 203,\n",
       " 'messages': 204,\n",
       " 'real': 205,\n",
       " 'nigga': 206,\n",
       " '✊“': 207,\n",
       " 'dang': 208,\n",
       " 'follower': 209,\n",
       " 'tonight': 210,\n",
       " 'lol”': 211,\n",
       " 'stop': 212,\n",
       " 'using': 213,\n",
       " 'word': 214,\n",
       " 'bouta': 215,\n",
       " 'down': 216,\n",
       " 'omg': 217,\n",
       " 'will': 218,\n",
       " '😂👌': 219,\n",
       " 'tomorrow': 220,\n",
       " 'tho': 221,\n",
       " '😒': 222,\n",
       " 'bye': 223,\n",
       " 'believe': 224,\n",
       " 'dreams': 225,\n",
       " 'neversaynever': 226,\n",
       " 'fsvmofq1gq': 227,\n",
       " 'tiff': 228,\n",
       " 'asleep': 229,\n",
       " 'sometimes': 230,\n",
       " 'usually': 231,\n",
       " 'if': 232,\n",
       " 'play': 233,\n",
       " 'basketball': 234,\n",
       " 'talk': 235,\n",
       " 'school': 236,\n",
       " 'visit': 237,\n",
       " 'next': 238,\n",
       " 'year': 239,\n",
       " 'orang': 240,\n",
       " 'malaysia': 241,\n",
       " 'tak': 242,\n",
       " 'payah': 243,\n",
       " 'nak': 244,\n",
       " 'tweet': 245,\n",
       " 'speak': 246,\n",
       " 'english': 247,\n",
       " 'bajet': 248,\n",
       " 'mat': 249,\n",
       " 'salleh': 250,\n",
       " 'excuse': 251,\n",
       " 'sir': 252,\n",
       " 'malaysian': 253,\n",
       " 'showing': 254,\n",
       " 'tooooo': 255,\n",
       " 'ma': 256,\n",
       " 'pants': 257,\n",
       " 'gotta': 258,\n",
       " 'gym': 259,\n",
       " 'literally': 260,\n",
       " \"couldn't\": 261,\n",
       " 'worse': 262,\n",
       " 'mood': 263,\n",
       " 'alot': 264,\n",
       " 'sittin': 265,\n",
       " 'thinkin': 266,\n",
       " 'spend': 267,\n",
       " 'thinking': 268,\n",
       " 'doin': 269,\n",
       " 'nothin': 270,\n",
       " 'too': 271,\n",
       " 'french': 272,\n",
       " 'cassettes': 273,\n",
       " 'great': 274,\n",
       " 'american': 275,\n",
       " 'hall': 276,\n",
       " '6s7iwk7t8c': 277,\n",
       " '“': 278,\n",
       " 'some': 279,\n",
       " 'tacosssss”omg': 280,\n",
       " 'hungry': 281,\n",
       " 'thestruggle': 282,\n",
       " 'pch': 283,\n",
       " 'sublime': 284,\n",
       " 'rome': 285,\n",
       " 'staying': 286,\n",
       " '50': 287,\n",
       " 'shades': 288,\n",
       " 'sit': 289,\n",
       " 'face': 290,\n",
       " 'did': 291,\n",
       " 'bring': 292,\n",
       " 'pokemon': 293,\n",
       " 'stickers': 294,\n",
       " 'care': 295,\n",
       " 'how': 296,\n",
       " 'long': 297,\n",
       " 'its': 298,\n",
       " 'counting': 299,\n",
       " 'days': 300,\n",
       " 'until': 301,\n",
       " 'january': 302,\n",
       " '2014': 303,\n",
       " 'bloood': 304,\n",
       " 'finnah': 305,\n",
       " 'brib': 306,\n",
       " 'call': 307,\n",
       " 'babe': 308,\n",
       " 'ohhhh': 309,\n",
       " 'kill': 310,\n",
       " 'em': 311,\n",
       " 'bitches': 312,\n",
       " 'got': 313,\n",
       " 'fucked': 314,\n",
       " 'saying': 315,\n",
       " \"jackie's\": 316,\n",
       " 'theirs': 317,\n",
       " 'everyone': 318,\n",
       " 'knows': 319,\n",
       " \"she's\": 320,\n",
       " 'clearly': 321,\n",
       " 'fucking': 322,\n",
       " 'longer': 323,\n",
       " 'than': 324,\n",
       " '✌': 325,\n",
       " 'games': 326,\n",
       " 'ages': 327,\n",
       " 'dont': 328,\n",
       " 'epik': 329,\n",
       " 'way': 330,\n",
       " 'wtf': 331,\n",
       " 'wait': 332,\n",
       " 'another': 333,\n",
       " 'hour': 334,\n",
       " 'cause': 335,\n",
       " 'account😡😡': 336,\n",
       " 'girl': 337,\n",
       " 'says': 338,\n",
       " 'smh': 339,\n",
       " 'eb5wegiupj': 340,\n",
       " 'rt': 341,\n",
       " 'these': 342,\n",
       " 'waffles': 343,\n",
       " 'y': 344,\n",
       " 'n': 345,\n",
       " 'auto': 346,\n",
       " 'correct': 347,\n",
       " 'strikes': 348,\n",
       " 'again': 349,\n",
       " 'mom': 350,\n",
       " 'try': 351,\n",
       " 'came': 352,\n",
       " 'disappointment': 353,\n",
       " 'youuu': 354,\n",
       " 'obvious': 355,\n",
       " 'haha': 356,\n",
       " 'yaaaa': 357,\n",
       " '😄😏': 358,\n",
       " 'rainy': 359,\n",
       " 'god': 360,\n",
       " 'cover': 361,\n",
       " 'our': 362,\n",
       " 'flight': 363,\n",
       " 'amen': 364,\n",
       " 'azwien': 365,\n",
       " 'kres': 366,\n",
       " 'setiawan': 367,\n",
       " 'others': 368,\n",
       " 'lcct': 369,\n",
       " 'kl': 370,\n",
       " 'international': 371,\n",
       " 'airport': 372,\n",
       " 'okuk4bb827': 373,\n",
       " 'feel': 374,\n",
       " 'obliged': 375,\n",
       " 'listen': 376,\n",
       " 'song': 377,\n",
       " 'considering': 378,\n",
       " 'fact': 379,\n",
       " 'live': 380,\n",
       " 'here': 381,\n",
       " 'waitng': 382,\n",
       " 'train': 383,\n",
       " 'show': 384,\n",
       " 'brickandmortarfreepour': 385,\n",
       " 'outpastmybedtime': 386,\n",
       " 'yes': 387,\n",
       " 'red': 388,\n",
       " 'eyes': 389,\n",
       " 'thanks': 390,\n",
       " 'info': 391,\n",
       " 'ate': 392,\n",
       " 'cheng': 393,\n",
       " '❤': 394,\n",
       " \"you're\": 395,\n",
       " 'hot': 396,\n",
       " 'why': 397,\n",
       " 'are': 398,\n",
       " 'douche': 399,\n",
       " 'fly': 400,\n",
       " 'kokapi': 401,\n",
       " 'beach': 402,\n",
       " 'ncjj6iwe39': 403,\n",
       " 'what': 404,\n",
       " 'fargo': 405,\n",
       " 'campaign': 406,\n",
       " 'boycott': 407,\n",
       " 'installation': 408,\n",
       " 'israeli': 409,\n",
       " 'water': 410,\n",
       " 'meters': 411,\n",
       " 'has': 412,\n",
       " 'launched': 413,\n",
       " 'lewes': 414,\n",
       " 'dxzocjqfhg': 415,\n",
       " 'killstreak': 416,\n",
       " '12': 417,\n",
       " 'even': 418,\n",
       " 'deserve': 419,\n",
       " 'djdrama': 420,\n",
       " 'headache': 421,\n",
       " 'ends': 422,\n",
       " '😱': 423,\n",
       " 'aio': 424,\n",
       " \"robotics'\": 425,\n",
       " 'cool': 426,\n",
       " 'teaser': 427,\n",
       " 'coming': 428,\n",
       " 'one': 429,\n",
       " 'machine': 430,\n",
       " 'zeus': 431,\n",
       " 'luwzsrotzg': 432,\n",
       " '3dprinting': 433,\n",
       " 'fabbing': 434,\n",
       " 'always': 435,\n",
       " 'wrong': 436,\n",
       " 'type': 437,\n",
       " 'indojassfest2013': 438,\n",
       " 'wendy': 439,\n",
       " 'astri': 440,\n",
       " 'maxebr7fsu': 441,\n",
       " 'clon': 442,\n",
       " 'doshermosas': 443,\n",
       " 'lassie': 444,\n",
       " 'iji9uhyfdx': 445,\n",
       " 'no': 446,\n",
       " 'part': 447,\n",
       " 'time': 448,\n",
       " '1st': 449,\n",
       " 'month😌': 450,\n",
       " 'trip': 451,\n",
       " 'exeter': 452,\n",
       " 'collect': 453,\n",
       " 'keys': 454,\n",
       " 'new': 455,\n",
       " 'definitely': 456,\n",
       " 'adult': 457,\n",
       " '👌🙈🔑🏠': 458,\n",
       " 'forgot': 459,\n",
       " 'heels': 460,\n",
       " 'can': 461,\n",
       " 'anything': 462,\n",
       " 'dedication': 463,\n",
       " 'conquer': 464,\n",
       " '🌍😊': 465,\n",
       " 'nighttimeinspiration': 466,\n",
       " 'sober': 467,\n",
       " 'thing': 468,\n",
       " 'not': 469,\n",
       " 'didnt': 470,\n",
       " 'answer': 471,\n",
       " 'job': 472,\n",
       " 'percy': 473,\n",
       " 'paragon': 474,\n",
       " 'xxi': 475,\n",
       " 'vygmgeafac': 476,\n",
       " 'twitters': 477,\n",
       " 'deaddddd': 478,\n",
       " 'where': 479,\n",
       " 'from': 480,\n",
       " 'found': 481,\n",
       " 'quietest': 482,\n",
       " 'starbucks': 483,\n",
       " 'ever': 484,\n",
       " 'phenomenal': 485,\n",
       " 'guys': 486,\n",
       " 'hats': 487,\n",
       " '♥♥♥♥♥': 488,\n",
       " 'kiddinggggg': 489,\n",
       " 'top': 490,\n",
       " 'sick': 491,\n",
       " 'tatum': 492,\n",
       " 'had': 493,\n",
       " 'fun': 494,\n",
       " 'marry': 495,\n",
       " 'bruno': 496,\n",
       " 'mars': 497,\n",
       " 'record': 498,\n",
       " 'rfrwl2tsq5': 499,\n",
       " 'nah': 500,\n",
       " 'chills': 501,\n",
       " 'hustler': 502,\n",
       " 'hudlin': 503,\n",
       " 'x': 504}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Unique tokens: {len(tokenizer.word_index)}\")\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(tweets.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 64)            64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=20))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 87 arrays: [array([[  1],\n       [  2],\n       [106],\n       [107],\n       [108],\n       [ 57],\n       [109]]), array([[  2],\n       [ 58],\n       [ 58],\n       [ 59],\n       [110],\n       [111],\n       [112],\n ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wtsp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wtsp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wtsp/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 87 arrays: [array([[  1],\n       [  2],\n       [106],\n       [107],\n       [108],\n       [ 57],\n       [109]]), array([[  2],\n       [ 58],\n       [ 58],\n       [ 59],\n       [110],\n       [111],\n       [112],\n ..."
     ]
    }
   ],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
