{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - Experiments\n",
    "\n",
    "This notebook contains AdHoc experiments with document representation using BERT/Transformer mechanisms then using them to perform document classification. This model will later be used to classify corpora of messages and dicern if they are related to a particular set of products and services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/models/keras artifacts/models/sklearn artifacts/results artifacts/results/logs artifacts/results/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MODIN_CPUS'] = \"10\"\n",
    "os.environ['MODIN_OUT_OF_CORE'] = \"true\"\n",
    "\n",
    "artifacts_path = os.path.join(os.path.curdir, 'artifacts/')\n",
    "models_path = os.path.join(artifacts_path, 'models/')\n",
    "sklearn_models = os.path.join(models_path, 'sklearn/')\n",
    "kears_modesl = os.path.join(models_path, 'keras/')\n",
    "results_path = os.path.join(artifacts_path, 'results/')\n",
    "logs_path = os.path.join(results_path, 'logs/')\n",
    "data_path = os.path.join(artifacts_path, 'data/')"
   ]
  },
  {
   "source": [
    "Perform the document tokenization using the pyspark script. It is much faster than pure python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "e Output Committer Algorithm version is 1\n",
      "20/11/22 16:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/22 16:56:23 INFO CodeGenerator: Code generated in 10.418887 ms\n",
      "20/11/22 16:56:23 INFO CodeGenerator: Code generated in 11.207922 ms\n",
      "20/11/22 16:56:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 287.5 KB, free 362.3 MB)\n",
      "20/11/22 16:56:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.9 KB, free 362.3 MB)\n",
      "20/11/22 16:56:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.110:37711 (size: 23.9 KB, free: 362.9 MB)\n",
      "20/11/22 16:56:23 INFO SparkContext: Created broadcast 6 from parquet at NativeMethodAccessorImpl.java:0\n",
      "20/11/22 16:56:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 45228050 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/11/22 16:56:23 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Missing parents: List()\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/11/22 16:56:23 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 159.8 KB, free 362.1 MB)\n",
      "20/11/22 16:56:23 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.6 KB, free 362.0 MB)\n",
      "20/11/22 16:56:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.110:37711 (size: 57.6 KB, free: 362.8 MB)\n",
      "20/11/22 16:56:23 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "20/11/22 16:56:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/11/22 16:56:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "20/11/22 16:56:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 214, localhost, executor driver, partition 0, PROCESS_LOCAL, 10879 bytes)\n",
      "20/11/22 16:56:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 214)\n",
      "20/11/22 16:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/11/22 16:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/22 16:56:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/11/22 16:56:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/22 16:56:23 INFO CodecConfig: Compression: SNAPPY\n",
      "20/11/22 16:56:23 INFO CodecConfig: Compression: SNAPPY\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Parquet page size to 1048576\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Dictionary is on\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Validation is off\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Page size checking is: estimated\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Min row count for page size check is: 100\n",
      "20/11/22 16:56:23 INFO ParquetOutputFormat: Max row count for page size check is: 10000\n",
      "20/11/22 16:56:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"categories\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"document\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tokenized_document\",\n",
      "    \"type\" : {\n",
      "      \"type\" : \"array\",\n",
      "      \"elementType\" : \"integer\",\n",
      "      \"containsNull\" : true\n",
      "    },\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary categories (UTF8);\n",
      "  optional binary document (UTF8);\n",
      "  optional group tokenized_document (LIST) {\n",
      "    repeated group list {\n",
      "      optional int32 element;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "       \n",
      "20/11/22 16:56:23 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "20/11/22 16:56:23 INFO CodeGenerator: Code generated in 6.660961 ms\n",
      "20/11/22 16:56:23 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00002-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:56:23 INFO CodeGenerator: Code generated in 7.340751 ms\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 97\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 81\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 78\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 66\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 49\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 62\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 120\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 109\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 94\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 116\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 96\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 95\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 48\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 56\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 67\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 52\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 104\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 50\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 58\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 85\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 83\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 80\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 102\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 107\n",
      "20/11/22 16:56:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.110:37711 in memory (size: 11.7 KB, free: 362.9 MB)\n",
      "20/11/22 16:56:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.110:37711 in memory (size: 12.1 KB, free: 362.9 MB)\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 98\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 79\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 69\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 75\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 65\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 121\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 90\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 54\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 99\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 76\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 84\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 47\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 72\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 106\n",
      "20/11/22 16:56:23 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.110:37711 in memory (size: 12.9 KB, free: 362.9 MB)\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 101\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 57\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 88\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 59\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 117\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 108\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 111\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 93\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 63\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 119\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 51\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 68\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 115\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 105\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 77\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 91\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 113\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 71\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 103\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 74\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 89\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 86\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 82\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 64\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 60\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 73\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 112\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 61\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 92\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 100\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 114\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 70\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 118\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 87\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 55\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 110\n",
      "20/11/22 16:56:23 INFO ContextCleaner: Cleaned accumulator 53\n",
      "20/11/22 16:56:38 INFO PythonUDFRunner: Times: total = 15266, boot = -1338, init = 1651, finish = 14953\n",
      "20/11/22 16:56:38 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00006-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:56:50 INFO InternalParquetRecordWriter: mem size 134795518 > 134217728: flushing 268848 records to disk.\n",
      "20/11/22 16:56:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134879914\n",
      "20/11/22 16:56:53 INFO PythonUDFRunner: Times: total = 14933, boot = -7, init = 113, finish = 14827\n",
      "20/11/22 16:56:53 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00000-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:57:09 INFO PythonUDFRunner: Times: total = 15732, boot = -41, init = 156, finish = 15617\n",
      "20/11/22 16:57:09 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00009-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:57:16 INFO InternalParquetRecordWriter: mem size 134250444 > 134217728: flushing 266954 records to disk.\n",
      "20/11/22 16:57:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134363185\n",
      "20/11/22 16:57:24 INFO PythonUDFRunner: Times: total = 15180, boot = -11, init = 104, finish = 15087\n",
      "20/11/22 16:57:24 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00005-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:57:40 INFO PythonUDFRunner: Times: total = 15457, boot = -9, init = 91, finish = 15375\n",
      "20/11/22 16:57:40 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00007-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:57:43 INFO InternalParquetRecordWriter: mem size 134653140 > 134217728: flushing 267930 records to disk.\n",
      "20/11/22 16:57:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134775013\n",
      "20/11/22 16:57:54 INFO PythonUDFRunner: Times: total = 14765, boot = -8, init = 89, finish = 14684\n",
      "20/11/22 16:57:54 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00008-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:58:09 INFO InternalParquetRecordWriter: mem size 134582236 > 134217728: flushing 269608 records to disk.\n",
      "20/11/22 16:58:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134660606\n",
      "20/11/22 16:58:10 INFO PythonUDFRunner: Times: total = 15929, boot = -43, init = 126, finish = 15846\n",
      "20/11/22 16:58:10 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00004-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:58:25 INFO PythonUDFRunner: Times: total = 15066, boot = -8, init = 90, finish = 14984\n",
      "20/11/22 16:58:25 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00001-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:58:36 INFO InternalParquetRecordWriter: mem size 134625587 > 134217728: flushing 269693 records to disk.\n",
      "20/11/22 16:58:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134812140\n",
      "20/11/22 16:58:41 INFO PythonUDFRunner: Times: total = 15216, boot = -43, init = 125, finish = 15134\n",
      "20/11/22 16:58:41 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00003-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO PythonUDFRunner: Times: total = 15488, boot = -42, init = 142, finish = 15388\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00008-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51256025, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00003-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51237741, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00000-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51099952, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00005-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-50767575, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00004-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-50058690, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO PythonUDFRunner: Times: total = 102, boot = -59, init = 161, finish = 0\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00009-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49880361, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00001-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49589189, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00002-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49217467, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00006-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49071428, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00007-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-48615143, partition values: [empty row]\n",
      "20/11/22 16:58:56 INFO PythonUDFRunner: Times: total = 52, boot = 4, init = 48, finish = 0\n",
      "20/11/22 16:58:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 105020171\n",
      "20/11/22 16:58:57 INFO FileOutputCommitter: Saved output of task 'attempt_20201122165623_0004_m_000000_214' to file:/home/ohtar10/git/wtsp/notebooks/jupyter/bert/artifacts/data/_temporary/0/task_20201122165623_0004_m_000000\n",
      "20/11/22 16:58:57 INFO SparkHadoopMapRedUtil: attempt_20201122165623_0004_m_000000_214: Committed\n",
      "20/11/22 16:58:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 214). 3241 bytes result sent to driver\n",
      "20/11/22 16:58:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 214) in 153753 ms on localhost (executor driver) (1/1)\n",
      "20/11/22 16:58:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "20/11/22 16:58:57 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 153.774 s\n",
      "20/11/22 16:58:57 INFO DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 153.776437 s\n",
      "20/11/22 16:58:57 INFO FileFormatWriter: Write Job d8729ff0-44b9-4064-8f9c-88f6fec80199 committed.\n",
      "20/11/22 16:58:57 INFO FileFormatWriter: Finished processing stats for write job d8729ff0-44b9-4064-8f9c-88f6fec80199.\n",
      "20/11/22 16:58:57 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/11/22 16:58:57 INFO SparkUI: Stopped Spark web UI at http://192.168.1.110:4041\n",
      "20/11/22 16:58:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/11/22 16:58:57 INFO MemoryStore: MemoryStore cleared\n",
      "20/11/22 16:58:57 INFO BlockManager: BlockManager stopped\n",
      "20/11/22 16:58:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/11/22 16:58:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/11/22 16:58:57 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/11/22 16:58:57 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/11/22 16:58:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-37c92c88-a2ad-4b85-92b4-5528cf9cd768/pyspark-4e03bd9c-6a9f-4efc-82de-97e484f319e6\n",
      "20/11/22 16:58:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d2c90a8-27a6-448c-ab2b-f6ce4342a9ef\n",
      "20/11/22 16:58:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-37c92c88-a2ad-4b85-92b4-5528cf9cd768\n",
      "CPU times: user 59.6 ms, sys: 0 ns, total: 59.6 ms\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "export raw_reviews=\"/media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/\"\n",
    "export current_dir=$(pwd)\n",
    "if [ ! -f \"artifacts/data/tokenized_docs.parquet\" ]; then\n",
    "    spark-submit ../../../dataprep/scripts/document_tokenizer.py --input \"${raw_reviews}\" --output \"${current_dir}/artifacts/data/\" --column document --vocab-size 200000 --maxlen 300 > spark.log\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f \"artifacts/data/tokenized_docs.parquet\" ]; then mv artifacts/data/*.parquet artifacts/data/tokenized_docs.parquet; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                              categories  \\\n",
       "0                                  Music   \n",
       "1                                  Books   \n",
       "2                 Health & Personal Care   \n",
       "3  Technology, Electronics & Accessories   \n",
       "4               Office & School Supplies   \n",
       "\n",
       "                                            document  \\\n",
       "0  Quality Radio App\\nI use this app a few times ...   \n",
       "1  Buy this Book!\\nAdmittedly I know the author o...   \n",
       "2  No Mess Jewelry Cleaner!\\nEasy to use Jewelry ...   \n",
       "3  IPad cover\\nThis cover is wonderful.  Well mad...   \n",
       "4  Great thin pen\\nLove the look and feel of this...   \n",
       "\n",
       "                                  tokenized_document  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categories</th>\n      <th>document</th>\n      <th>tokenized_document</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Music</td>\n      <td>Quality Radio App\\nI use this app a few times ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Books</td>\n      <td>Buy this Book!\\nAdmittedly I know the author o...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Health &amp; Personal Care</td>\n      <td>No Mess Jewelry Cleaner!\\nEasy to use Jewelry ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Technology, Electronics &amp; Accessories</td>\n      <td>IPad cover\\nThis cover is wonderful.  Well mad...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Office &amp; School Supplies</td>\n      <td>Great thin pen\\nLove the look and feel of this...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "\n",
    "reviews_path = os.path.join(data_path, 'tokenized_docs.parquet')\n",
    "reviews = pd.read_parquet(reviews_path, columns=['categories', 'document', 'tokenized_document'], engine=\"pyarrow\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total documents: 1,553,620\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents: {len(reviews):,}\")"
   ]
  },
  {
   "source": [
    "### Category binarizer\n",
    "\n",
    "This is a simple category binarizer to encode the document categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = reviews['categories'].apply(lambda cat: cat.split(\";\")).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "categories_encoder_path = os.path.join(sklearn_models, 'category_encoder.pkl')\n",
    "categories_encoder = None\n",
    "if os.path.exists(categories_encoder_path):\n",
    "    with open(categories_encoder_path, 'rb') as file:\n",
    "        categories_encoder = pickle.load(file)\n",
    "else:\n",
    "    categories_encoder = MultiLabelBinarizer()\n",
    "    categories_encoder.fit(categories)\n",
    "    with open(categories_encoder_path, 'wb') as file:\n",
    "        pickle.dump(categories_encoder, file)"
   ]
  },
  {
   "source": [
    "### Defining a baseline to beat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "flat_categories = [item for sublist in categories for item in sublist]\n",
    "count = Counter(flat_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Books', 457672),\n",
       " ('Technology, Electronics & Accessories', 269065),\n",
       " ('Home & Kitchen', 249313),\n",
       " ('Clothing, Shoes & Jewelry', 162550),\n",
       " ('Health & Personal Care', 125661),\n",
       " ('Toys & Games', 117803),\n",
       " ('Sports & Outdoors', 104651),\n",
       " ('Music', 82883),\n",
       " ('Movies & TV', 80190),\n",
       " ('Office & School Supplies', 28248)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sorted(count.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline: 0.295\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline: {count['Books'] / len(categories):.3f}\")"
   ]
  },
  {
   "source": [
    "### Creating a stratified sample\n",
    "\n",
    "While testing different model versions, first let's have a stratified sample of the dataset. In this case, we are going to use only 8% of the data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stratified_sample(dataset: pd.DataFrame, classes: np.ndarray, fraction: float, seed: int = None, class_col: str = 'categories') -> pd.DataFrame:\n",
    "    samples = []\n",
    "    for c in classes:\n",
    "        if seed:\n",
    "            samples.append(dataset[dataset[class_col].str.contains(c)].sample(frac=fraction, random_state=seed))\n",
    "        else:\n",
    "            samples.append(dataset[dataset[class_col].str.contains(c)].sample(frac=fraction))\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        categories                                           document  \\\n",
       "889468       Books  Just damn good\\nThe book is just really, reall...   \n",
       "894813       Books  Looks Are Very Deceiving\\nMs. Weldon wrote a c...   \n",
       "1329162      Books  Great book\\nThis book was most helpful in just...   \n",
       "1424706      Books  Breath Taking\\nI loved this book. I read it on...   \n",
       "861722       Books  Daughter of Joy\\nI loved this book and the mes...   \n",
       "\n",
       "                                        tokenized_document  \n",
       "889468   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "894813   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1329162  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1424706  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "861722   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categories</th>\n      <th>document</th>\n      <th>tokenized_document</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>889468</th>\n      <td>Books</td>\n      <td>Just damn good\\nThe book is just really, reall...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>894813</th>\n      <td>Books</td>\n      <td>Looks Are Very Deceiving\\nMs. Weldon wrote a c...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1329162</th>\n      <td>Books</td>\n      <td>Great book\\nThis book was most helpful in just...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1424706</th>\n      <td>Books</td>\n      <td>Breath Taking\\nI loved this book. I read it on...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>861722</th>\n      <td>Books</td>\n      <td>Daughter of Joy\\nI loved this book and the mes...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sample = stratified_sample(reviews, categories_encoder.classes_, 0.008, 123)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample size: 13,424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample size: {len(sample):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Books', 3782),\n",
       " ('Technology, Electronics & Accessories', 2616),\n",
       " ('Home & Kitchen', 2413),\n",
       " ('Clothing, Shoes & Jewelry', 1650),\n",
       " ('Sports & Outdoors', 1233),\n",
       " ('Health & Personal Care', 1091),\n",
       " ('Toys & Games', 984),\n",
       " ('Music', 714),\n",
       " ('Movies & TV', 693),\n",
       " ('Office & School Supplies', 229)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "sample_categories = sample.categories.apply(lambda cat: cat.split(\";\")).values.tolist()\n",
    "flat_categories = [item for sublist in sample_categories for item in sublist]\n",
    "count = Counter(flat_categories)\n",
    "sorted(count.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "source": [
    "This smaller sample preserves the proportion of the original data set. We can use this to try out different models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Preparing artifacts for training and evaluation\n",
    "\n",
    "With the spark script above, we tokenized and saved a word index python dict that we can import in a sklearn based document tokenizer. We don't need to re-tokenize since the document is already tokenized. However, we will load the word index in case we want to tokenize new entries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class DocumentTokenizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, corpus_column: str, \n",
    "                lowercase: bool = True, \n",
    "                tokenizer=None, \n",
    "                vocab_size=None, \n",
    "                maxlen=None,\n",
    "                word_index=None):\n",
    "        self.corpus_column = corpus_column\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        if word_index:\n",
    "            self.word_index = word_index\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        word_count = X[self.corpus_column].apply(\n",
    "                lambda corpus: self.tokenizer.tokenize(corpus.lower() if self.lowercase else corpus)\n",
    "            ).explode().value_counts().sort_values(ascending=False)\n",
    "\n",
    "        if self.vocab_size:\n",
    "            word_count = word_count.iloc[0:self.vocab_size]\n",
    "        word_index = word_count.reset_index()['index'].to_dict()\n",
    "        self.word_index = {v:k for k, v in word_index.items()}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def tokenize(string):\n",
    "            tokens = self.tokenizer.tokenize(string.lower())\n",
    "            tokens = [self.word_index[token] for token in tokens if token in self.word_index]\n",
    "            if self.maxlen:\n",
    "                tokens = tokens[:self.maxlen]\n",
    "            return tokens\n",
    "\n",
    "        X[f'tokenized_{self.corpus_column}'] = X[self.corpus_column].apply(tokenize)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.01 ms, sys: 145 µs, total: 1.16 ms\nWall time: 708 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fit_tokenizer = False\n",
    "doc_tokenizer_path = os.path.join(sklearn_models, 'document_tokenizer.pkl')\n",
    "\n",
    "if os.path.exists(doc_tokenizer_path):\n",
    "    with open(doc_tokenizer_path, 'rb') as file:\n",
    "        doc_tokenizer = pickle.load(file)\n",
    "else:\n",
    "    if fit_tokenizer:\n",
    "        doc_tokenizer = DocumentTokenizer(corpus_column='document', vocab_size=vocab_size, maxlen=maxlen)\n",
    "        doc_tokenizer.fit(sample)\n",
    "    else:\n",
    "        word_index = os.path.join(data_path, 'word_index.pkl')\n",
    "        doc_tokenizer = DocumentTokenizer(corpus_column='document', vocab_size=vocab_size, maxlen=maxlen, word_index=word_index)\n",
    "    with open(doc_tokenizer_path, 'wb') as file:\n",
    "         pickle.dump(doc_tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_docs = sample\n",
    "# Undoment this code if you want to re-tokenize the documents\n",
    "\n",
    "# tokenized_docs_path = os.path.join(data_path, 'tokenized_docs.parquet')\n",
    "# if os.path.exists(tokenized_docs_path):\n",
    "#     tokenized_docs = pd.read_parquet(tokenized_docs_path, columns=['categories', 'document', 'tokenized_document'], engine='pyarrow')\n",
    "# else:\n",
    "#     tokenized_docs = doc_tokenizer.transform(sample)\n",
    "#     tokenized_docs.to_parquet(tokenized_docs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokenized documents: 13,424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokenized documents: {len(tokenized_docs):,}\")"
   ]
  },
  {
   "source": [
    "## Defining the Attention based text classifier\n",
    "\n",
    "Resource: https://keras.io/examples/nlp/text_classification_with_transformer/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "### Implement multi head self attention as Keras layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimmension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        return output\n"
   ]
  },
  {
   "source": [
    "### Implement a Transformer block as a layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation='relu'),\n",
    "                layers.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "source": [
    "### Implement embedding layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "source": [
    "### Create classifier model using transformer layer\n",
    "Transformer layer outputs one vector for each time step of our input sequence. Here, we take the mean across all time steps and use a feed forward network on top of it to classify text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 300)]             0         \n_________________________________________________________________\ntoken_and_position_embedding (None, 300, 128)          25638400  \n_________________________________________________________________\ntransformer_block (Transform (None, 300, 128)          99584     \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 128)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 256)               33024     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 25,805,194\nTrainable params: 25,805,194\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 # Embedding size for each token\n",
    "num_heads = 8 # number of attention heads\n",
    "ff_dim = 128 # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Note: these below were already defined a some cells above. Redefinding them to remember them\n",
    "maxlen = maxlen # Only consider the first 300 words of each product review\n",
    "vocab_size = vocab_size # Only consider the top 200k words\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(len(categories_encoder.classes_), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Prepare the data for the experiment from the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Prepare the labels using the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total categories: 13424 with size: 10 each\n"
     ]
    }
   ],
   "source": [
    "sample_categories = tokenized_docs['categories'].apply(lambda cat: cat.split(\";\")).values.tolist()\n",
    "y = categories_encoder.transform(sample_categories)\n",
    "print(f\"Total categories: {len(y)} with size: {len(y[0])} each\")"
   ]
  },
  {
   "source": [
    "Prepare the encoded corpora from the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total documents for training: 13424, with sequence length: 300\nCPU times: user 3min 6s, sys: 19.4 s, total: 3min 25s\nWall time: 53min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = os.path.join(data_path, 'train_data.npz')\n",
    "if not os.path.exists(train_data_path):\n",
    "    X = tokenized_docs['tokenized_document'].values\n",
    "    X = np.array(X.tolist())\n",
    "    np.savez_compressed(train_data_path, X=X)\n",
    "else:\n",
    "    X = np.load(train_data_path)['X']\n",
    "\n",
    "print(f\"Total documents for training: {len(X)}, with sequence length: {len(X[0])}\")"
   ]
  },
  {
   "source": [
    "Prepare training, dev, and test stratified sample sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n"
   ]
  },
  {
   "source": [
    "### Train and evaluate the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 12081 samples, validate on 671 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_block/multi_head_self_attention/dense_1/kernel:0', 'transformer_block/multi_head_self_attention/dense_1/bias:0', 'transformer_block/multi_head_self_attention/dense_2/kernel:0', 'transformer_block/multi_head_self_attention/dense_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_block/multi_head_self_attention/dense_1/kernel:0', 'transformer_block/multi_head_self_attention/dense_1/bias:0', 'transformer_block/multi_head_self_attention/dense_2/kernel:0', 'transformer_block/multi_head_self_attention/dense_2/bias:0'] when minimizing the loss.\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 2.0633 - accuracy: 0.3595WARNING:tensorflow:From /home/ohtar10/miniconda3/envs/wtsp/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 2.0628 - accuracy: 0.3597 - val_loss: 1.6381 - val_accuracy: 0.4739\n",
      "Epoch 2/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 1.5039 - accuracy: 0.5393INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 78s 6ms/sample - loss: 1.5036 - accuracy: 0.5394 - val_loss: 1.3144 - val_accuracy: 0.5902\n",
      "Epoch 3/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 1.1422 - accuracy: 0.6767INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 81s 7ms/sample - loss: 1.1429 - accuracy: 0.6766 - val_loss: 1.2316 - val_accuracy: 0.6423\n",
      "Epoch 4/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.9226 - accuracy: 0.7522INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.9225 - accuracy: 0.7521 - val_loss: 1.1381 - val_accuracy: 0.6826\n",
      "Epoch 5/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.7558 - accuracy: 0.8108INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 81s 7ms/sample - loss: 0.7556 - accuracy: 0.8109 - val_loss: 1.5463 - val_accuracy: 0.5350\n",
      "Epoch 6/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.8261INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 81s 7ms/sample - loss: 0.6795 - accuracy: 0.8261 - val_loss: 1.3533 - val_accuracy: 0.6498\n",
      "Epoch 7/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.8563INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 79s 7ms/sample - loss: 0.5668 - accuracy: 0.8564 - val_loss: 1.2949 - val_accuracy: 0.6930\n",
      "Epoch 8/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.8699INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.5187 - accuracy: 0.8701 - val_loss: 1.3250 - val_accuracy: 0.6826\n",
      "Epoch 9/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.8801INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.4769 - accuracy: 0.8802 - val_loss: 1.4738 - val_accuracy: 0.6870\n",
      "Epoch 10/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.8908INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.4387 - accuracy: 0.8909 - val_loss: 1.5389 - val_accuracy: 0.6811\n",
      "Epoch 11/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4048 - accuracy: 0.8919INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.4045 - accuracy: 0.8921 - val_loss: 1.6920 - val_accuracy: 0.6870\n",
      "Epoch 12/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.9042INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.3700 - accuracy: 0.9041 - val_loss: 1.8621 - val_accuracy: 0.6453\n",
      "Epoch 13/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3870 - accuracy: 0.8991INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.3872 - accuracy: 0.8991 - val_loss: 1.4726 - val_accuracy: 0.6781\n",
      "Epoch 14/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.9051INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.3598 - accuracy: 0.9051 - val_loss: 1.4561 - val_accuracy: 0.7004\n",
      "Epoch 15/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3505 - accuracy: 0.8982INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 81s 7ms/sample - loss: 0.3508 - accuracy: 0.8983 - val_loss: 1.3515 - val_accuracy: 0.7004\n",
      "Epoch 16/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.9088INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 82s 7ms/sample - loss: 0.3226 - accuracy: 0.9089 - val_loss: 1.9407 - val_accuracy: 0.6811\n",
      "Epoch 17/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.9081INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 80s 7ms/sample - loss: 0.3257 - accuracy: 0.9078 - val_loss: 1.7045 - val_accuracy: 0.6826\n",
      "Epoch 18/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.9063INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 79s 7ms/sample - loss: 0.3442 - accuracy: 0.9064 - val_loss: 1.6663 - val_accuracy: 0.6647\n",
      "Epoch 19/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9068INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 79s 7ms/sample - loss: 0.3034 - accuracy: 0.9068 - val_loss: 1.9699 - val_accuracy: 0.6304\n",
      "Epoch 20/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.9103INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 78s 6ms/sample - loss: 0.3386 - accuracy: 0.9104 - val_loss: 1.7627 - val_accuracy: 0.6662\n",
      "CPU times: user 1h 43min 6s, sys: 17min 50s, total: 2h 57s\n",
      "Wall time: 26min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "checkpoints_path = os.path.join(results_path, 'checkpoints/')\n",
    "logs_path = os.path.join(results_path, 'logs/')\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=5, min_delta=1e-7, restore_best_weights=True),\n",
    "    TensorBoard(log_dir=logs_path),\n",
    "    ModelCheckpoint(checkpoints_path, monitor='val_acc')\n",
    "]\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "# losses\n",
    "# kullback_leibler_divergence\n",
    "# categorical_hinge\n",
    "# categorical_crossentropy\n",
    "model.compile(optimizer=\"adam\", loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_dev, y_dev), callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.keras.models.load_model(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "672/672 [==============================] - 0s 537us/sample - loss: 1.8869 - accuracy: 0.6786\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.8868522360211326, 0.6785714]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('wtsp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "60f1df135fa14e0a8cd6f1cba451775870a2013e1541c01995e720fc6b462c84"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}