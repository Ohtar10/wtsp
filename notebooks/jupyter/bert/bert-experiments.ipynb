{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - Experiments\n",
    "\n",
    "This notebook contains AdHoc experiments with document representation using BERT/Transformer mechanisms then using them to perform document classification. This model will later be used to classify corpora of messages and dicern if they are related to a particular set of products and services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p artifacts/models/keras artifacts/models/sklearn artifacts/results artifacts/results/logs artifacts/results/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MODIN_CPUS'] = \"10\"\n",
    "os.environ['MODIN_OUT_OF_CORE'] = \"true\"\n",
    "\n",
    "artifacts_path = os.path.join(os.path.curdir, 'artifacts/')\n",
    "models_path = os.path.join(artifacts_path, 'models/')\n",
    "sklearn_models = os.path.join(models_path, 'sklearn/')\n",
    "kears_modesl = os.path.join(models_path, 'keras/')\n",
    "results_path = os.path.join(artifacts_path, 'results/')\n",
    "logs_path = os.path.join(results_path, 'logs/')\n",
    "data_path = os.path.join(artifacts_path, 'data/')"
   ]
  },
  {
   "source": [
    "Perform the document tokenization using the pyspark script. It is much faster than pure python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/26 18:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/11/26 18:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/26 18:25:49 INFO CodeGenerator: Code generated in 7.470777 ms\n",
      "20/11/26 18:25:49 INFO CodeGenerator: Code generated in 8.837825 ms\n",
      "20/11/26 18:25:49 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 287.5 KB, free 362.3 MB)\n",
      "20/11/26 18:25:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.110:44875 in memory (size: 12.1 KB, free: 362.9 MB)\n",
      "20/11/26 18:25:49 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.9 KB, free 362.3 MB)\n",
      "20/11/26 18:25:49 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.110:44875 (size: 23.9 KB, free: 362.9 MB)\n",
      "20/11/26 18:25:49 INFO SparkContext: Created broadcast 6 from parquet at NativeMethodAccessorImpl.java:0\n",
      "20/11/26 18:25:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 45228050 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/11/26 18:25:49 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Missing parents: List()\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/11/26 18:25:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.6 KB, free 362.2 MB)\n",
      "20/11/26 18:25:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.4 KB, free 362.1 MB)\n",
      "20/11/26 18:25:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.110:44875 (size: 57.4 KB, free: 362.9 MB)\n",
      "20/11/26 18:25:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "20/11/26 18:25:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/11/26 18:25:49 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "20/11/26 18:25:49 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 214, localhost, executor driver, partition 0, PROCESS_LOCAL, 10879 bytes)\n",
      "20/11/26 18:25:49 INFO Executor: Running task 0.0 in stage 4.0 (TID 214)\n",
      "20/11/26 18:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/11/26 18:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/26 18:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/11/26 18:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/11/26 18:25:49 INFO CodecConfig: Compression: SNAPPY\n",
      "20/11/26 18:25:49 INFO CodecConfig: Compression: SNAPPY\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Parquet page size to 1048576\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Dictionary is on\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Validation is off\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Page size checking is: estimated\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Min row count for page size check is: 100\n",
      "20/11/26 18:25:49 INFO ParquetOutputFormat: Max row count for page size check is: 10000\n",
      "20/11/26 18:25:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"categories\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"document\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tokenized_document\",\n",
      "    \"type\" : {\n",
      "      \"type\" : \"array\",\n",
      "      \"elementType\" : \"integer\",\n",
      "      \"containsNull\" : true\n",
      "    },\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary categories (UTF8);\n",
      "  optional binary document (UTF8);\n",
      "  optional group tokenized_document (LIST) {\n",
      "    repeated group list {\n",
      "      optional int32 element;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "       \n",
      "20/11/26 18:25:49 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "20/11/26 18:25:49 INFO CodeGenerator: Code generated in 6.643963 ms\n",
      "20/11/26 18:25:49 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00002-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:25:49 INFO CodeGenerator: Code generated in 7.010608 ms\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 77\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 59\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 54\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 116\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 100\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 81\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 110\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 90\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 76\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 75\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 51\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 98\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 50\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 60\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 67\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 88\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 94\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 103\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 105\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 120\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 68\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 108\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 95\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 62\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 52\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 112\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 102\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 118\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 111\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 114\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 96\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 66\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 89\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 113\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 61\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 86\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 93\n",
      "20/11/26 18:25:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.110:44875 in memory (size: 12.9 KB, free: 362.9 MB)\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 117\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 97\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 64\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 101\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 71\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 83\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 57\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 107\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 99\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 119\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 106\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 85\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 73\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 78\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 91\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 58\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 121\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 74\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 79\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 63\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 48\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 104\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 109\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 53\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 84\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 72\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 82\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 115\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 65\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 56\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 70\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 49\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 55\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 69\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 87\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 80\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 92\n",
      "20/11/26 18:25:50 INFO ContextCleaner: Cleaned accumulator 47\n",
      "20/11/26 18:25:59 INFO PythonUDFRunner: Times: total = 9070, boot = -1300, init = 1426, finish = 8944\n",
      "20/11/26 18:25:59 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00006-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:06 INFO InternalParquetRecordWriter: mem size 134402084 > 134217728: flushing 260944 records to disk.\n",
      "20/11/26 18:26:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134717725\n",
      "20/11/26 18:26:09 INFO PythonUDFRunner: Times: total = 8844, boot = -801, init = 911, finish = 8734\n",
      "20/11/26 18:26:09 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00000-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:19 INFO PythonUDFRunner: Times: total = 8989, boot = -788, init = 888, finish = 8889\n",
      "20/11/26 18:26:19 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00009-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:22 INFO InternalParquetRecordWriter: mem size 134441142 > 134217728: flushing 259972 records to disk.\n",
      "20/11/26 18:26:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134822337\n",
      "20/11/26 18:26:28 INFO PythonUDFRunner: Times: total = 8967, boot = -742, init = 842, finish = 8867\n",
      "20/11/26 18:26:28 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00005-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:38 INFO PythonUDFRunner: Times: total = 8964, boot = -784, init = 861, finish = 8887\n",
      "20/11/26 18:26:38 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00007-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:39 INFO InternalParquetRecordWriter: mem size 134931310 > 134217728: flushing 260938 records to disk.\n",
      "20/11/26 18:26:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 135205588\n",
      "20/11/26 18:26:47 INFO PythonUDFRunner: Times: total = 8680, boot = -732, init = 806, finish = 8606\n",
      "20/11/26 18:26:47 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00008-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:26:55 INFO InternalParquetRecordWriter: mem size 134320042 > 134217728: flushing 261832 records to disk.\n",
      "20/11/26 18:26:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134590312\n",
      "20/11/26 18:26:57 INFO PythonUDFRunner: Times: total = 9167, boot = -754, init = 851, finish = 9070\n",
      "20/11/26 18:26:57 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00004-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:27:07 INFO PythonUDFRunner: Times: total = 8605, boot = -788, init = 865, finish = 8528\n",
      "20/11/26 18:27:07 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00001-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:27:11 INFO InternalParquetRecordWriter: mem size 134552510 > 134217728: flushing 261816 records to disk.\n",
      "20/11/26 18:27:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134756569\n",
      "20/11/26 18:27:16 INFO PythonUDFRunner: Times: total = 8601, boot = -843, init = 917, finish = 8527\n",
      "20/11/26 18:27:16 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00003-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 0-45228050, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO PythonUDFRunner: Times: total = 8727, boot = -806, init = 889, finish = 8644\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00008-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51256025, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00003-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51237741, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00000-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-51099952, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00005-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-50767575, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00004-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-50058690, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO PythonUDFRunner: Times: total = 108, boot = -773, init = 881, finish = 0\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00009-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49880361, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00001-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49589189, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00002-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49217467, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00006-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-49071428, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO FileScanRDD: Reading File path: file:///media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/part-00007-05cb1305-3925-4d26-8f49-5f5ba817c28e-c000.snappy.parquet, range: 45228050-48615143, partition values: [empty row]\n",
      "20/11/26 18:27:26 INFO PythonUDFRunner: Times: total = 60, boot = 4, init = 56, finish = 0\n",
      "20/11/26 18:27:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 127375047\n",
      "20/11/26 18:27:26 INFO FileOutputCommitter: Saved output of task 'attempt_20201126182549_0004_m_000000_214' to file:/home/ohtar10/git/wtsp/notebooks/jupyter/bert/artifacts/data/_temporary/0/task_20201126182549_0004_m_000000\n",
      "20/11/26 18:27:26 INFO SparkHadoopMapRedUtil: attempt_20201126182549_0004_m_000000_214: Committed\n",
      "20/11/26 18:27:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 214). 3241 bytes result sent to driver\n",
      "20/11/26 18:27:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 214) in 96915 ms on localhost (executor driver) (1/1)\n",
      "20/11/26 18:27:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "20/11/26 18:27:26 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 96.936 s\n",
      "20/11/26 18:27:26 INFO DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 96.938650 s\n",
      "20/11/26 18:27:26 INFO FileFormatWriter: Write Job 2494bf47-7006-467f-919e-5e0cb65be0d0 committed.\n",
      "20/11/26 18:27:26 INFO FileFormatWriter: Finished processing stats for write job 2494bf47-7006-467f-919e-5e0cb65be0d0.\n",
      "20/11/26 18:27:26 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/11/26 18:27:26 INFO SparkUI: Stopped Spark web UI at http://192.168.1.110:4041\n",
      "20/11/26 18:27:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/11/26 18:27:26 INFO MemoryStore: MemoryStore cleared\n",
      "20/11/26 18:27:26 INFO BlockManager: BlockManager stopped\n",
      "20/11/26 18:27:26 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/11/26 18:27:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/11/26 18:27:26 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/11/26 18:27:26 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/11/26 18:27:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-995ae8df-fe91-43b4-a358-876f1193faff\n",
      "20/11/26 18:27:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-46b468db-d6bf-415b-8bad-34a283f94167\n",
      "20/11/26 18:27:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-46b468db-d6bf-415b-8bad-34a283f94167/pyspark-cc70ba14-8a31-4681-94ef-c5c4c7c14d73\n",
      "CPU times: user 51.8 ms, sys: 3.96 ms, total: 55.7 ms\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "export raw_reviews=\"/media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-small-shuffle/\"\n",
    "export current_dir=$(pwd)\n",
    "if [ ! -f \"artifacts/data/tokenized_docs.parquet\" ]; then\n",
    "    spark-submit ../../../dataprep/scripts/document_tokenizer.py --input \"${raw_reviews}\" --output \"${current_dir}/artifacts/data/\" --column document --vocab-size 200000 --maxlen 300 --batches 1 > spark.log\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export current_dir=$(pwd)\n",
    "if [ ! -f \"artifacts/data/tokenized_docs.parquet\" ]; then\n",
    "    mv artifacts/data/*.parquet artifacts/data/tokenized_docs.parquet \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                              categories  \\\n",
       "0                                  Music   \n",
       "1                                  Books   \n",
       "2                 Health & Personal Care   \n",
       "3  Technology, Electronics & Accessories   \n",
       "4               Office & School Supplies   \n",
       "\n",
       "                                            document  \\\n",
       "0  Quality Radio App\\nI use this app a few times ...   \n",
       "1  Buy this Book!\\nAdmittedly I know the author o...   \n",
       "2  No Mess Jewelry Cleaner!\\nEasy to use Jewelry ...   \n",
       "3  IPad cover\\nThis cover is wonderful.  Well mad...   \n",
       "4  Great thin pen\\nLove the look and feel of this...   \n",
       "\n",
       "                                  tokenized_document  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categories</th>\n      <th>document</th>\n      <th>tokenized_document</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Music</td>\n      <td>Quality Radio App\\nI use this app a few times ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Books</td>\n      <td>Buy this Book!\\nAdmittedly I know the author o...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Health &amp; Personal Care</td>\n      <td>No Mess Jewelry Cleaner!\\nEasy to use Jewelry ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Technology, Electronics &amp; Accessories</td>\n      <td>IPad cover\\nThis cover is wonderful.  Well mad...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Office &amp; School Supplies</td>\n      <td>Great thin pen\\nLove the look and feel of this...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "\n",
    "reviews_path = os.path.join(data_path, 'tokenized_docs.parquet')\n",
    "reviews = pd.read_parquet(reviews_path, columns=['categories', 'document', 'tokenized_document'], engine=\"pyarrow\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total documents: 1,553,620\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents: {len(reviews):,}\")"
   ]
  },
  {
   "source": [
    "### Category binarizer\n",
    "\n",
    "This is a simple category binarizer to encode the document categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = reviews['categories'].apply(lambda cat: cat.split(\";\")).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "\n",
    "categories_encoder_path = os.path.join(sklearn_models, 'category_encoder.pkl')\n",
    "categories_encoder = None\n",
    "if os.path.exists(categories_encoder_path):\n",
    "    with open(categories_encoder_path, 'rb') as file:\n",
    "        categories_encoder = pickle.load(file)\n",
    "else:\n",
    "    categories_encoder = MultiLabelBinarizer()\n",
    "    categories_encoder.fit(categories)\n",
    "    with open(categories_encoder_path, 'wb') as file:\n",
    "        pickle.dump(categories_encoder, file)"
   ]
  },
  {
   "source": [
    "### Defining a baseline to beat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "flat_categories = [item for sublist in categories for item in sublist]\n",
    "count = Counter(flat_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Books', 457672),\n",
       " ('Technology, Electronics & Accessories', 269065),\n",
       " ('Home & Kitchen', 249313),\n",
       " ('Clothing, Shoes & Jewelry', 162550),\n",
       " ('Health & Personal Care', 125661),\n",
       " ('Toys & Games', 117803),\n",
       " ('Sports & Outdoors', 104651),\n",
       " ('Music', 82883),\n",
       " ('Movies & TV', 80190),\n",
       " ('Office & School Supplies', 28248)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sorted(count.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline: 0.295\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline: {count['Books'] / len(categories):.3f}\")"
   ]
  },
  {
   "source": [
    "### Creating a stratified sample\n",
    "\n",
    "While testing different model versions, first let's have a stratified sample of the dataset. In this case, we are going to use only 8% of the data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stratified_sample(dataset: pd.DataFrame, classes: np.ndarray, fraction: float, seed: int = None, class_col: str = 'categories') -> pd.DataFrame:\n",
    "    samples = []\n",
    "    for c in classes:\n",
    "        if seed:\n",
    "            samples.append(dataset[dataset[class_col].str.contains(c)].sample(frac=fraction, random_state=seed))\n",
    "        else:\n",
    "            samples.append(dataset[dataset[class_col].str.contains(c)].sample(frac=fraction))\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        categories                                           document  \\\n",
       "889468       Books  Just damn good\\nThe book is just really, reall...   \n",
       "894813       Books  Looks Are Very Deceiving\\nMs. Weldon wrote a c...   \n",
       "1329162      Books  Great book\\nThis book was most helpful in just...   \n",
       "1424706      Books  Breath Taking\\nI loved this book. I read it on...   \n",
       "861722       Books  Daughter of Joy\\nI loved this book and the mes...   \n",
       "\n",
       "                                        tokenized_document  \n",
       "889468   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "894813   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1329162  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1424706  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "861722   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>categories</th>\n      <th>document</th>\n      <th>tokenized_document</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>889468</th>\n      <td>Books</td>\n      <td>Just damn good\\nThe book is just really, reall...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>894813</th>\n      <td>Books</td>\n      <td>Looks Are Very Deceiving\\nMs. Weldon wrote a c...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1329162</th>\n      <td>Books</td>\n      <td>Great book\\nThis book was most helpful in just...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1424706</th>\n      <td>Books</td>\n      <td>Breath Taking\\nI loved this book. I read it on...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>861722</th>\n      <td>Books</td>\n      <td>Daughter of Joy\\nI loved this book and the mes...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sample = stratified_sample(reviews, categories_encoder.classes_, 0.008, 123)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample size: 13,424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample size: {len(sample):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Books', 3782),\n",
       " ('Technology, Electronics & Accessories', 2616),\n",
       " ('Home & Kitchen', 2413),\n",
       " ('Clothing, Shoes & Jewelry', 1650),\n",
       " ('Sports & Outdoors', 1233),\n",
       " ('Health & Personal Care', 1091),\n",
       " ('Toys & Games', 984),\n",
       " ('Music', 714),\n",
       " ('Movies & TV', 693),\n",
       " ('Office & School Supplies', 229)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sample_categories = sample.categories.apply(lambda cat: cat.split(\";\")).values.tolist()\n",
    "flat_categories = [item for sublist in sample_categories for item in sublist]\n",
    "count = Counter(flat_categories)\n",
    "sorted(count.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "source": [
    "This smaller sample preserves the proportion of the original data set. We can use this to try out different models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Preparing artifacts for training and evaluation\n",
    "\n",
    "With the spark script above, we tokenized and saved a word index python dict that we can import in a sklearn based document tokenizer. We don't need to re-tokenize since the document is already tokenized. However, we will load the word index in case we want to tokenize new entries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 200000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class DocumentTokenizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, corpus_column: str, \n",
    "                lowercase: bool = True, \n",
    "                tokenizer=None, \n",
    "                vocab_size=None, \n",
    "                maxlen=None,\n",
    "                word_index=None):\n",
    "        self.corpus_column = corpus_column\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        if word_index:\n",
    "            self.word_index = word_index\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        word_count = X[self.corpus_column].apply(\n",
    "                lambda corpus: self.tokenizer.tokenize(corpus.lower() if self.lowercase else corpus)\n",
    "            ).explode().value_counts().sort_values(ascending=False)\n",
    "\n",
    "        if self.vocab_size:\n",
    "            word_count = word_count.iloc[0:self.vocab_size]\n",
    "        word_index = word_count.reset_index()['index'].to_dict()\n",
    "        self.word_index = {v:k for k, v in word_index.items()}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def tokenize(string):\n",
    "            tokens = self.tokenizer.tokenize(string.lower())\n",
    "            tokens = [self.word_index[token] for token in tokens if token in self.word_index]\n",
    "            if self.maxlen:\n",
    "                tokens = tokens[:self.maxlen]\n",
    "            return tokens\n",
    "\n",
    "        X[f'tokenized_{self.corpus_column}'] = X[self.corpus_column].apply(tokenize)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 0 ns, sys: 1.18 ms, total: 1.18 ms\nWall time: 670 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fit_tokenizer = False\n",
    "doc_tokenizer_path = os.path.join(sklearn_models, 'document_tokenizer.pkl')\n",
    "\n",
    "if os.path.exists(doc_tokenizer_path):\n",
    "    with open(doc_tokenizer_path, 'rb') as file:\n",
    "        doc_tokenizer = pickle.load(file)\n",
    "else:\n",
    "    if fit_tokenizer:\n",
    "        doc_tokenizer = DocumentTokenizer(corpus_column='document', vocab_size=vocab_size, maxlen=maxlen)\n",
    "        doc_tokenizer.fit(sample)\n",
    "    else:\n",
    "        word_index = os.path.join(data_path, 'word_index.pkl')\n",
    "        doc_tokenizer = DocumentTokenizer(corpus_column='document', vocab_size=vocab_size, maxlen=maxlen, word_index=word_index)\n",
    "    with open(doc_tokenizer_path, 'wb') as file:\n",
    "         pickle.dump(doc_tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\nWall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_docs = sample\n",
    "# Undoment this code if you want to re-tokenize the documents\n",
    "\n",
    "# tokenized_docs_path = os.path.join(data_path, 'tokenized_docs.parquet')\n",
    "# if os.path.exists(tokenized_docs_path):\n",
    "#     tokenized_docs = pd.read_parquet(tokenized_docs_path, columns=['categories', 'document', 'tokenized_document'], engine='pyarrow')\n",
    "# else:\n",
    "#     tokenized_docs = doc_tokenizer.transform(sample)\n",
    "#     tokenized_docs.to_parquet(tokenized_docs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokenized documents: 13,424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokenized documents: {len(tokenized_docs):,}\")"
   ]
  },
  {
   "source": [
    "## Defining the Attention based text classifier\n",
    "\n",
    "Resource: https://keras.io/examples/nlp/text_classification_with_transformer/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "### Implement multi head self attention as Keras layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimmension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs) # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            query, batch_size\n",
    "        ) # (batch_size, num_heads, seq_len, projection_dim)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        ) # (batch_size, seq_len, embed_dim)\n",
    "        return output\n"
   ]
  },
  {
   "source": [
    "### Implement a Transformer block as a layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation='relu'),\n",
    "                layers.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "source": [
    "### Implement embedding layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "source": [
    "### Create classifier model using transformer layer\n",
    "Transformer layer outputs one vector for each time step of our input sequence. Here, we take the mean across all time steps and use a feed forward network on top of it to classify text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         [(None, 300)]             0         \n_________________________________________________________________\ntoken_and_position_embedding (None, 300, 256)          51276800  \n_________________________________________________________________\ntransformer_block_5 (Transfo (None, 300, 256)          330112    \n_________________________________________________________________\nglobal_average_pooling1d_5 ( (None, 256)               0         \n_________________________________________________________________\ndropout_22 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_51 (Dense)             (None, 512)               131584    \n_________________________________________________________________\ndropout_23 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_52 (Dense)             (None, 216)               110808    \n_________________________________________________________________\ndense_53 (Dense)             (None, 10)                2170      \n=================================================================\nTotal params: 51,851,474\nTrainable params: 51,851,474\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256 # Embedding size for each token\n",
    "num_heads = 2 # number of attention heads\n",
    "ff_dim = 128 # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Note: these below were already defined a some cells above. Redefinding them to remember them\n",
    "maxlen = maxlen # Only consider the first 300 words of each product review\n",
    "vocab_size = vocab_size # Only consider the top 200k words\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(216, activation='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(len(categories_encoder.classes_), activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Prepare the data for the experiment from the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Prepare the labels using the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total categories: 13424 with size: 10 each\n"
     ]
    }
   ],
   "source": [
    "sample_categories = tokenized_docs['categories'].apply(lambda cat: cat.split(\";\")).values.tolist()\n",
    "y = categories_encoder.transform(sample_categories)\n",
    "print(f\"Total categories: {len(y)} with size: {len(y[0])} each\")"
   ]
  },
  {
   "source": [
    "Prepare the encoded corpora from the sample sub set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total documents for training: 13424, with sequence length: 300\nCPU times: user 3min 29s, sys: 23.8 s, total: 3min 52s\nWall time: 1h 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = os.path.join(data_path, 'train_data.npz')\n",
    "if not os.path.exists(train_data_path):\n",
    "    X = tokenized_docs['tokenized_document'].values\n",
    "    X = np.array(X.tolist())\n",
    "    np.savez_compressed(train_data_path, X=X)\n",
    "else:\n",
    "    X = np.load(train_data_path)['X']\n",
    "\n",
    "print(f\"Total documents for training: {len(X)}, with sequence length: {len(X[0])}\")"
   ]
  },
  {
   "source": [
    "Prepare training, dev, and test stratified sample sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n"
   ]
  },
  {
   "source": [
    "### Train and evaluate the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 12081 samples, validate on 671 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_block_5/multi_head_self_attention_5/dense_46/kernel:0', 'transformer_block_5/multi_head_self_attention_5/dense_46/bias:0', 'transformer_block_5/multi_head_self_attention_5/dense_47/kernel:0', 'transformer_block_5/multi_head_self_attention_5/dense_47/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_block_5/multi_head_self_attention_5/dense_46/kernel:0', 'transformer_block_5/multi_head_self_attention_5/dense_46/bias:0', 'transformer_block_5/multi_head_self_attention_5/dense_47/kernel:0', 'transformer_block_5/multi_head_self_attention_5/dense_47/bias:0'] when minimizing the loss.\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 2.0599 - accuracy: 0.3604INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 145s 12ms/sample - loss: 2.0594 - accuracy: 0.3608 - val_loss: 1.6490 - val_accuracy: 0.4650\n",
      "Epoch 2/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 1.4275 - accuracy: 0.5695INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 141s 12ms/sample - loss: 1.4268 - accuracy: 0.5697 - val_loss: 1.2068 - val_accuracy: 0.6557\n",
      "Epoch 3/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 1.0682 - accuracy: 0.7099INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 1.0679 - accuracy: 0.7099 - val_loss: 1.0765 - val_accuracy: 0.6915\n",
      "Epoch 4/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.8846 - accuracy: 0.7724INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.8842 - accuracy: 0.7725 - val_loss: 1.2649 - val_accuracy: 0.6796\n",
      "Epoch 5/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.7311 - accuracy: 0.8164INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.7310 - accuracy: 0.8165 - val_loss: 1.1175 - val_accuracy: 0.7064\n",
      "Epoch 6/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.6301 - accuracy: 0.8433INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.6303 - accuracy: 0.8431 - val_loss: 1.1992 - val_accuracy: 0.7019\n",
      "Epoch 7/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.8527INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 142s 12ms/sample - loss: 0.5807 - accuracy: 0.8527 - val_loss: 1.2175 - val_accuracy: 0.7034\n",
      "Epoch 8/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4963 - accuracy: 0.8770INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.4960 - accuracy: 0.8771 - val_loss: 1.3856 - val_accuracy: 0.6617\n",
      "Epoch 9/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.8800INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.4863 - accuracy: 0.8801 - val_loss: 1.3930 - val_accuracy: 0.6885\n",
      "Epoch 10/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4514 - accuracy: 0.8845INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.4513 - accuracy: 0.8845 - val_loss: 1.4231 - val_accuracy: 0.6975\n",
      "Epoch 11/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.4542 - accuracy: 0.8838INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 142s 12ms/sample - loss: 0.4539 - accuracy: 0.8838 - val_loss: 1.3518 - val_accuracy: 0.7139\n",
      "Epoch 12/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8953INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.3983 - accuracy: 0.8954 - val_loss: 1.6137 - val_accuracy: 0.6811\n",
      "Epoch 13/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.9053INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.3697 - accuracy: 0.9053 - val_loss: 1.3010 - val_accuracy: 0.7109\n",
      "Epoch 14/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.9005INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.3782 - accuracy: 0.9006 - val_loss: 1.4058 - val_accuracy: 0.7630\n",
      "Epoch 15/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.9012INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.3655 - accuracy: 0.9009 - val_loss: 1.3427 - val_accuracy: 0.7213\n",
      "Epoch 16/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3381 - accuracy: 0.9077INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.3378 - accuracy: 0.9079 - val_loss: 1.4112 - val_accuracy: 0.7228\n",
      "Epoch 17/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.9054INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.3622 - accuracy: 0.9054 - val_loss: 1.4181 - val_accuracy: 0.7183\n",
      "Epoch 18/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.9140INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 144s 12ms/sample - loss: 0.3256 - accuracy: 0.9141 - val_loss: 1.5847 - val_accuracy: 0.7034\n",
      "Epoch 19/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.9153INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.3156 - accuracy: 0.9153 - val_loss: 1.4783 - val_accuracy: 0.7064\n",
      "Epoch 20/20\n",
      "12064/12081 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.9134INFO:tensorflow:Assets written to: ./artifacts/results/checkpoints/assets\n",
      "12081/12081 [==============================] - 143s 12ms/sample - loss: 0.2932 - accuracy: 0.9134 - val_loss: 1.7851 - val_accuracy: 0.6736\n",
      "CPU times: user 3h 20min 58s, sys: 30min 42s, total: 3h 51min 40s\n",
      "Wall time: 47min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "checkpoints_path = os.path.join(results_path, 'checkpoints/')\n",
    "logs_path = os.path.join(results_path, 'logs/')\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=5, min_delta=1e-7, restore_best_weights=True),\n",
    "    TensorBoard(log_dir=logs_path),\n",
    "    ModelCheckpoint(checkpoints_path, monitor='val_acc')\n",
    "]\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "# losses\n",
    "# kullback_leibler_divergence\n",
    "# categorical_hinge\n",
    "# categorical_crossentropy\n",
    "model.compile(optimizer=\"adam\", loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_dev, y_dev), callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.keras.models.load_model(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "672/672 [==============================] - 0s 537us/sample - loss: 1.8869 - accuracy: 0.6786\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.8868522360211326, 0.6785714]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('wtsp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "60f1df135fa14e0a8cd6f1cba451775870a2013e1541c01995e720fc6b462c84"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}