{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('wtsp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "60f1df135fa14e0a8cd6f1cba451775870a2013e1541c01995e720fc6b462c84"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"/media/ohtar10/Adder-Storage/datasets/pre-processed/product-documents-tiny\"\n",
    "\n",
    "artifacts_path = os.path.join(os.path.curdir, 'artifacts/')\n",
    "models_path = os.path.join(artifacts_path, 'models/')\n",
    "sklearn_models = os.path.join(models_path, 'sklearn/')\n",
    "data_path = os.path.join(artifacts_path, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "categories_encoder_path = os.path.join(sklearn_models, 'category_encoder.pkl')\n",
    "categories_encoder = None\n",
    "with open(categories_encoder_path, 'rb') as file:\n",
    "    categories_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class DocumentTokenizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, corpus_column: str, \n",
    "                lowercase: bool = True,\n",
    "                vocab_size=None, \n",
    "                maxlen=None,\n",
    "                word_index=None):\n",
    "        self.corpus_column = corpus_column\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        if word_index:\n",
    "            with open(word_index, 'rb') as file:\n",
    "                self.word_index = pickle.load(file)\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        word_count = X[self.corpus_column].apply(\n",
    "                lambda corpus: self.tokenizer.tokenize(corpus.lower() if self.lowercase else corpus)\n",
    "            ).explode().value_counts().sort_values(ascending=False)\n",
    "\n",
    "        if self.vocab_size:\n",
    "            word_count = word_count.iloc[0:self.vocab_size]\n",
    "        word_index = word_count.reset_index()['index'].to_dict()\n",
    "        self.word_index = {v:k for k, v in word_index.items()}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def tokenize(row):\n",
    "            tokens = [self.word_index[token] for token in row if token in self.word_index]\n",
    "            return [0] * (self.maxlen - len(tokens)) + tokens[:self.maxlen]\n",
    "        \n",
    "        words = X[self.corpus_column].str.lower().str.split(f'[^\\w+]', expand=True)\n",
    "        return words.apply(tokenize, axis=1, result_type='expand').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_path = os.path.join(data_path, 'word_index.pkl')\n",
    "doc_tokenizer = DocumentTokenizer(\"document\", maxlen=300, word_index=word_index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import glob\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DocumentDataGenerator(Sequence):\n",
    "    def __init__(self, path, \n",
    "        doc_column, \n",
    "        cat_column,\n",
    "        category_encoder,\n",
    "        tokenizer,\n",
    "        shuffle=True, \n",
    "        to_fit=True):\n",
    "        self.path = path\n",
    "        self.doc_column = doc_column\n",
    "        self.cat_column = cat_column\n",
    "        self.category_encoder = category_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.shuffle = shuffle\n",
    "        self.to_fit = to_fit\n",
    "        self.fileset = [file for file in glob.glob(f\"{path}/*.parquet\", recursive=True)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.fileset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        df = pd.read_parquet(self.fileset[index])\n",
    "        X = self.tokenizer.transform(df)\n",
    "        y = df[self.cat_column].apply(lambda cat: cat.split(\";\")).values.tolist()\n",
    "        y = self.category_encoder.transform(y)\n",
    "\n",
    "        if self.to_fit:\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DocumentDataGenerator(base_path, 'document', 'categories', categories_encoder, doc_tokenizer)\n",
    "testing_generator = DocumentDataGenerator(base_path, 'document', 'categories', categories_encoder, doc_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='kullback_leibler_divergence', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 10 steps, validate for 10 steps\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 378s 38s/step - loss: 15.2598 - accuracy: 0.1294 - val_loss: 14.1213 - val_accuracy: 0.2031\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 374s 37s/step - loss: 13.3768 - accuracy: 0.2503 - val_loss: 12.8028 - val_accuracy: 0.2866\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 372s 37s/step - loss: 12.7474 - accuracy: 0.2901 - val_loss: 12.7201 - val_accuracy: 0.2919\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 393s 39s/step - loss: 12.7164 - accuracy: 0.2921 - val_loss: 12.7126 - val_accuracy: 0.2924\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 379s 38s/step - loss: 12.7114 - accuracy: 0.2925 - val_loss: 12.7110 - val_accuracy: 0.2925\n",
      "CPU times: user 32min 22s, sys: 32.7 s, total: 32min 55s\n",
      "Wall time: 32min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(training_generator, validation_data=testing_generator,  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}